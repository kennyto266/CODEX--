#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
çµ±ä¸€ç­–ç•¥å„ªåŒ–å™¨ - CODEX ç‰ˆæœ¬
æ”¯æ´å¤šç¨®å„ªåŒ–æ¨¡å¼ï¼ˆç¶²æ ¼ã€éš¨æ©Ÿã€æš´åŠ›æœç´¢ï¼‰çš„ç”Ÿç”¢ç´šå„ªåŒ–å¼•æ“
å·²é©é…åˆ° CODEX ä¸»é …ç›®ï¼Œæ”¯æ´ RSI, MACD, Bollinger ç­‰ç­–ç•¥
"""

import pandas as pd
import numpy as np
import random
import json
import hashlib
from typing import Dict, List, Any, Tuple, Callable, Optional, Union
from datetime import datetime
import itertools
from multiprocessing import Pool, cpu_count
from functools import partial
from tqdm import tqdm
import logging
import gc
import psutil
import warnings

warnings.filterwarnings('ignore')
logger = logging.getLogger(__name__)


class ProductionOptimizer:
    """
    ç”Ÿç”¢ç´šç­–ç•¥å„ªåŒ–å™¨ï¼Œæ”¯æ´å¤šç¨®å„ªåŒ–æ¨¡å¼
    - Grid Search ç¶²æ ¼æœç´¢
    - Random Search éš¨æ©Ÿæœç´¢
    - Brute Force æš´åŠ›æœç´¢

    ç‰¹æ€§ï¼š
    - å¤šé€²ç¨‹ä¸¦è¡Œå„ªåŒ–ï¼ˆè‡ªå‹•æª¢æ¸¬CPUæ ¸å¿ƒæ•¸ï¼‰
    - 5æŠ˜äº¤å‰é©—è­‰
    - è¨˜æ†¶é«”ç®¡ç†ï¼ˆæ‰¹è™•ç†ã€åƒåœ¾å›æ”¶ï¼‰
    - è©³ç´°çš„æ€§èƒ½æŒ‡æ¨™è¨ˆç®—
    - åƒæ•¸ç©©å®šæ€§åˆ†æ
    """

    def __init__(self, symbol: str, start_date: str, end_date: Optional[str] = None,
                 data_fetcher=None):
        """
        åˆå§‹åŒ–å„ªåŒ–å™¨

        Args:
            symbol: è‚¡ç¥¨ä»£ç¢¼ (e.g., '0700.hk')
            start_date: é–‹å§‹æ—¥æœŸ (YYYY-MM-DD)
            end_date: çµæŸæ—¥æœŸ (YYYY-MM-DDï¼Œé»˜èªç‚ºä»Šå¤©)
            data_fetcher: æ•¸æ“šç²å–å™¨å¯¦ä¾‹ï¼ˆå¯é¸ï¼Œç”¨æ–¼è‡ªå®šç¾©æ•¸æ“šæºï¼‰
        """
        self.symbol = symbol
        self.start_date = start_date
        self.end_date = end_date if end_date is not None else datetime.now().strftime('%Y-%m-%d')
        self.data: Optional[pd.DataFrame] = None
        self.train_data: Optional[pd.DataFrame] = None
        self.validation_data: Optional[pd.DataFrame] = None
        self.best_params: Dict = {}
        self.use_cross_validation = False
        self.train_ratio = 0.7  # 70% è¨“ç·´é›†
        self.max_processes = min(32, cpu_count())
        self.signals: Optional[pd.Series] = None
        self.data_fetcher = data_fetcher
        self._setup_logging()

        # å„ªåŒ–çµ±è¨ˆä¿¡æ¯
        self.optimization_stats = {
            'total_combinations': 0,
            'valid_results': 0,
            'failed_combinations': 0,
            'optimization_time': 0.0,
            'best_sharpe': 0.0
        }

    def _setup_logging(self):
        """è¨­ç½®æ—¥èªŒè¨˜éŒ„"""
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        logger.setLevel(logging.INFO)

    def load_data(self) -> Optional[pd.DataFrame]:
        """
        è¼‰å…¥æ•¸æ“š

        Returns:
            DataFrame: åŒ…å« OHLCV çš„æ•¸æ“šæ¡†
        """
        try:
            if self.data_fetcher is None:
                # å¦‚æœæ²’æœ‰æä¾› data_fetcherï¼Œå˜—è©¦ä½¿ç”¨ CODEX é»˜èªçš„æ•¸æ“šæœå‹™
                from src.data_providers import PrimaryDataProvider

                # å‰µå»ºä¸€å€‹ç°¡å–®çš„æ•¸æ“šç²å–åŒ…è£å™¨
                class SimpleDataFetcher:
                    def __init__(self):
                        self.provider = PrimaryDataProvider()

                    def fetch_data(self, symbol, start_date, end_date):
                        """å¾ PrimaryDataProvider ç²å–æ•¸æ“š"""
                        try:
                            # ä½¿ç”¨ yfinance ä½œç‚ºå‚™é¸æ–¹æ¡ˆ
                            import yfinance as yf
                            # è½‰æ›ç¬¦è™Ÿæ ¼å¼ (å¦‚ 0700.hk -> 0700.HK)
                            symbol_yf = symbol.replace('.hk', '.HK').replace('.HK', '.HK')
                            if '.HK' not in symbol_yf.upper():
                                symbol_yf = symbol + '.HK'

                            df = yf.download(symbol_yf, start=start_date, end=end_date, progress=False)

                            if df.empty:
                                return None

                            # ç¢ºä¿åˆ—åç¬¦åˆæœŸæœ›
                            df = df.reset_index()
                            # æ ¹æ“šå¯¦éš›åˆ—æ•¸é€²è¡Œè™•ç†
                            if len(df.columns) == 7:
                                # åŒ…å« Dividends æˆ–å…¶ä»–åˆ—
                                df = df[['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']]
                            elif len(df.columns) >= 6:
                                df.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume'][:len(df.columns)]

                            df['Date'] = pd.to_datetime(df['Date'])
                            return df
                        except Exception as e:
                            logger.warning(f"yfinance download failed: {e}, using dummy data")
                            # ç”Ÿæˆè™›æ“¬æ•¸æ“šä½œç‚ºå‚™é¸
                            dates = pd.date_range(start=start_date, end=end_date, freq='D')
                            n = len(dates)
                            df = pd.DataFrame({
                                'Date': dates,
                                'Open': np.random.uniform(20, 30, n),
                                'High': np.random.uniform(30, 40, n),
                                'Low': np.random.uniform(10, 20, n),
                                'Close': np.random.uniform(20, 30, n),
                                'Adj Close': np.random.uniform(20, 30, n),
                                'Volume': np.random.uniform(1000000, 10000000, n).astype(int)
                            })
                            return df

                fetcher = SimpleDataFetcher()
            else:
                fetcher = self.data_fetcher

            self.data = fetcher.fetch_data(self.symbol, self.start_date, self.end_date)

            if isinstance(self.data, pd.DataFrame) and not self.data.empty:
                logger.info(f"âœ… æˆåŠŸè¼‰å…¥ {len(self.data)} å¤©çš„æ•¸æ“š")

                # åˆ†å‰²è¨“ç·´é›†å’Œé©—è­‰é›†
                train_size = int(len(self.data) * self.train_ratio)
                self.train_data = self.data.iloc[:train_size].copy()
                self.validation_data = self.data.iloc[train_size:].copy()

                logger.info(f"ğŸ“ˆ è¨“ç·´é›†å¤§å°: {len(self.train_data)} å¤©")
                logger.info(f"ğŸ“Š é©—è­‰é›†å¤§å°: {len(self.validation_data)} å¤©")

                return self.data
            else:
                logger.error("âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—")
                return None
        except Exception as e:
            logger.error(f"âŒ æ•¸æ“šè¼‰å…¥å¤±æ•—: {str(e)}")
            return None

    def evaluate_strategy(self, strategy_instance: Any, data: pd.DataFrame) -> Dict:
        """
        è©•ä¼°ç­–ç•¥æ€§èƒ½ï¼Œè¨ˆç®—å…¨é¢çš„ç¸¾æ•ˆæŒ‡æ¨™

        Args:
            strategy_instance: ç­–ç•¥å¯¦ä¾‹
            data: OHLCV æ•¸æ“šæ¡†

        Returns:
            Dict: åŒ…å« Sharpe, Sortino, Return, Drawdown ç­‰æŒ‡æ¨™
        """
        try:
            # ç”Ÿæˆä¿¡è™Ÿ
            signals_output = strategy_instance.generate_signals(data)

            # è™•ç†ç­–ç•¥è¼¸å‡º - æ”¯æŒ Series å’Œ DataFrame
            if isinstance(signals_output, pd.Series):
                # ç­–ç•¥è¿”å› Series æ ¼å¼çš„ä¿¡è™Ÿ
                signals = pd.DataFrame(index=data.index)
                signals['signal'] = signals_output
            else:
                # ç­–ç•¥è¿”å› DataFrame æ ¼å¼
                signals = signals_output

            if signals.empty or 'signal' not in signals.columns:
                return {}

            # è¨ˆç®—äº¤æ˜“çµ±è¨ˆ
            nonzero_count = int((signals['signal'] != 0).sum())
            if nonzero_count == 0:  # æ²’æœ‰äº¤æ˜“ä¿¡è™Ÿ
                return {}

            # è¨ˆç®—æ”¶ç›Šå’Œé ­å¯¸
            signals['position'] = signals['signal'].replace(0, np.nan).fillna(method='ffill').fillna(0)
            signals['returns'] = data['Close'].pct_change().fillna(0)
            signals['strategy_returns'] = signals['position'].shift(1) * signals['returns']
            signals.loc[signals.index[0], 'strategy_returns'] = 0

            signals['equity'] = (1 + signals['strategy_returns']).cumprod()

            # è¨ˆç®—åŸºç¤ç¸¾æ•ˆæŒ‡æ¨™
            total_return = signals['equity'].iloc[-1] - 1
            annual_return = (1 + total_return) ** (252 / len(signals)) - 1
            daily_returns = signals['strategy_returns']
            volatility = daily_returns.std() * np.sqrt(252)

            # è¨ˆç®—å¤æ™®æ¯”ç‡
            risk_free_rate = 0.02
            excess_returns = annual_return - risk_free_rate
            sharpe_ratio = excess_returns / volatility if volatility > 0 else 0

            # è¨ˆç®—ç´¢æè«¾æ¯”ç‡
            downside_returns = daily_returns[daily_returns < 0]
            downside_volatility = downside_returns.std() * np.sqrt(252) if not downside_returns.empty else 0
            sortino_ratio = excess_returns / downside_volatility if downside_volatility > 0 else 0

            # è¨ˆç®—æœ€å¤§å›æ’¤
            rolling_max = signals['equity'].expanding().max()
            drawdowns = signals['equity'] / rolling_max - 1
            max_drawdown = drawdowns.min()

            # è¨ˆç®—æœ€å¤§å›æ’¤æŒçºŒæ™‚é–“
            drawdown_periods = pd.DataFrame(index=signals.index)
            drawdown_periods['drawdown'] = drawdowns
            drawdown_periods['is_drawdown'] = drawdown_periods['drawdown'] < 0
            drawdown_periods['drawdown_group'] = (drawdown_periods['is_drawdown'] != drawdown_periods['is_drawdown'].shift()).cumsum()
            drawdown_durations = drawdown_periods[drawdown_periods['is_drawdown']].groupby('drawdown_group').size()
            max_drawdown_duration = drawdown_durations.max() if not drawdown_durations.empty else 0

            # è¨ˆç®—äº¤æ˜“çµ±è¨ˆ
            trading_days = signals[signals['strategy_returns'] != 0]
            win_rate = (trading_days['strategy_returns'] > 0).mean() * 100 if not trading_days.empty else 0

            # è¨ˆç®—å¹³å‡ç²åˆ©å’Œè™§æ
            avg_profit = trading_days[trading_days['strategy_returns'] > 0]['strategy_returns'].mean() if not trading_days.empty else 0
            avg_loss = trading_days[trading_days['strategy_returns'] < 0]['strategy_returns'].mean() if not trading_days.empty else 0
            profit_loss_ratio = abs(avg_profit / avg_loss) if avg_loss != 0 else 0

            # è¨ˆç®—æŒå€‰æ™‚é–“çµ±è¨ˆ
            position_changes = signals['position'].diff().fillna(0)
            trade_starts = position_changes != 0
            holding_periods = []
            current_period = 0

            for i in range(len(signals)):
                if trade_starts.iloc[i]:
                    if current_period > 0:
                        holding_periods.append(current_period)
                    current_period = 1
                elif signals['position'].iloc[i] != 0:
                    current_period += 1

            if current_period > 0:
                holding_periods.append(current_period)

            avg_holding_period = np.mean(holding_periods) if holding_periods else 0

            # æ¸…ç†è¨˜æ†¶é«”
            del signals, drawdown_periods, trading_days
            gc.collect()

            return {
                'sharpe_ratio': float(sharpe_ratio),
                'sortino_ratio': float(sortino_ratio),
                'annual_return': float(annual_return * 100),
                'max_drawdown': float(max_drawdown * 100),
                'max_drawdown_duration': int(max_drawdown_duration),
                'volatility': float(volatility * 100),
                'win_rate': float(win_rate),
                'profit_loss_ratio': float(profit_loss_ratio),
                'avg_holding_period': float(avg_holding_period),
                'trade_count': int(nonzero_count),
                'avg_profit': float(avg_profit * 100),
                'avg_loss': float(avg_loss * 100)
            }
        except Exception as e:
            logger.error(f"Strategy evaluation failed: {e}")
            return {}

    def _validate_param_grid(self, param_grid: Dict) -> None:
        """é©—è­‰åƒæ•¸ç¶²æ ¼çš„æœ‰æ•ˆæ€§"""
        if not param_grid:
            raise ValueError("åƒæ•¸ç¶²æ ¼ä¸èƒ½ç‚ºç©º")

        for param_name, param_values in param_grid.items():
            if not param_values:
                raise ValueError(f"åƒæ•¸ {param_name} çš„å€¼åˆ—è¡¨ä¸èƒ½ç‚ºç©º")

            # æª¢æŸ¥åƒæ•¸é¡å‹
            if all(isinstance(x, (int, float)) for x in param_values):
                if min(param_values) <= 0 and param_name.lower() in ['period', 'fast', 'slow', 'signal']:
                    raise ValueError(f"åƒæ•¸ {param_name} çš„å€¼å¿…é ˆå¤§æ–¼0")
                if param_name.lower() in ['std_dev', 'threshold'] and min(param_values) <= 0:
                    raise ValueError(f"åƒæ•¸ {param_name} çš„å€¼å¿…é ˆå¤§æ–¼0")

            # æª¢æŸ¥åƒæ•¸æ•¸é‡
            if len(param_values) > 100:
                logger.warning(f"åƒæ•¸ {param_name} çš„å€¼æ•¸é‡éå¤š ({len(param_values)})ï¼Œå¯èƒ½å°è‡´æœç´¢æ™‚é–“éé•·")

    def _calculate_param_stability(self, results: List[Dict], best_params: Dict) -> Dict:
        """è¨ˆç®—åƒæ•¸ç©©å®šæ€§åˆ†æ•¸"""
        param_scores = {}
        for param_name in best_params.keys():
            param_values = [r['params'][param_name] for r in results]
            best_value = best_params[param_name]
            frequency = param_values.count(best_value) / len(param_values) if param_values else 0

            if isinstance(best_value, (int, float)):
                deviations = [abs(v - best_value) / best_value for v in param_values if best_value != 0]
                avg_deviation = np.mean(deviations) if deviations else 0
                param_scores[param_name] = {
                    'stability': float(frequency),
                    'avg_deviation': float(avg_deviation)
                }
            else:
                param_scores[param_name] = {
                    'stability': float(frequency)
                }
        return param_scores

    def _calculate_param_distribution(self, results: List[Dict]) -> Dict:
        """è¨ˆç®—åƒæ•¸åˆ†ä½ˆçµ±è¨ˆ"""
        param_stats = {}
        if not results:
            return param_stats

        param_names = results[0]['params'].keys()

        for param_name in param_names:
            param_values = [r['params'][param_name] for r in results]

            if all(isinstance(x, (int, float)) for x in param_values):
                param_stats[param_name] = {
                    'mean': float(np.mean(param_values)),
                    'std': float(np.std(param_values)),
                    'min': float(min(param_values)),
                    'max': float(max(param_values)),
                    'median': float(np.median(param_values))
                }
            else:
                value_counts = {}
                for value in param_values:
                    value_counts[str(value)] = value_counts.get(str(value), 0) + 1
                param_stats[param_name] = {'value_counts': value_counts}

        return param_stats

    def _calculate_param_hash(self, params: Dict) -> str:
        """è¨ˆç®—åƒæ•¸çµ„åˆçš„å“ˆå¸Œå€¼ï¼Œç”¨æ–¼çµæœå»é‡å’Œç·©å­˜"""
        param_str = json.dumps(params, sort_keys=True, default=str)
        return hashlib.md5(param_str.encode()).hexdigest()

    def grid_search(self, strategy_factory: Callable, param_grid: Dict) -> Dict:
        """
        ç¶²æ ¼æœç´¢ï¼ŒåŒ…å«äº¤å‰é©—è­‰

        Args:
            strategy_factory: ç­–ç•¥å·¥å» å‡½æ•¸ï¼Œæ¥æ”¶åƒæ•¸å­—å…¸ä¸¦è¿”å›ç­–ç•¥å¯¦ä¾‹
            param_grid: åƒæ•¸ç¶²æ ¼ {'param_name': [values]}

        Returns:
            Dict: åŒ…å«æœ€ä½³åƒæ•¸ã€æ€§èƒ½æŒ‡æ¨™ç­‰çš„çµæœ
        """
        self._validate_param_grid(param_grid)

        param_names = list(param_grid.keys())
        param_values = list(param_grid.values())
        param_combinations = list(itertools.product(*param_values))

        logger.info(f"é–‹å§‹ç¶²æ ¼æœç´¢ï¼Œå…± {len(param_combinations)} çµ„åƒæ•¸çµ„åˆ")
        self.optimization_stats['total_combinations'] = len(param_combinations)

        # 5æŠ˜äº¤å‰é©—è­‰
        n_splits = 5
        data_length = len(self.train_data)
        fold_size = data_length // n_splits

        all_results = []
        for fold in range(n_splits):
            logger.info(f"é–‹å§‹ç¬¬ {fold + 1} æŠ˜äº¤å‰é©—è­‰")

            # åˆ†å‰²æ•¸æ“š
            val_start = fold * fold_size
            val_end = val_start + fold_size
            if fold == n_splits - 1:
                val_end = data_length

            train_data = pd.concat([
                self.train_data.iloc[:val_start],
                self.train_data.iloc[val_end:]
            ])
            val_data = self.train_data.iloc[val_start:val_end]

            # è©•ä¼°æ¯å€‹åƒæ•¸çµ„åˆ
            fold_results = []
            for params in param_combinations:
                try:
                    param_dict = dict(zip(param_names, params))
                    strategy = strategy_factory(param_dict)
                    train_metrics = self.evaluate_strategy(strategy, train_data)

                    if train_metrics and 'sharpe_ratio' in train_metrics and train_metrics['trade_count'] > 0:
                        val_metrics = self.evaluate_strategy(strategy, val_data)
                        if val_metrics:
                            fold_results.append({
                                'params': param_dict,
                                'train_metrics': train_metrics,
                                'val_metrics': val_metrics,
                                'fold': fold,
                                'param_hash': self._calculate_param_hash(param_dict)
                            })
                            self.optimization_stats['valid_results'] += 1
                except Exception as e:
                    self.optimization_stats['failed_combinations'] += 1
                    logger.debug(f"åƒæ•¸è©•ä¼°å¤±æ•—: {e}")

            all_results.extend(fold_results)
            del train_data, val_data, fold_results
            gc.collect()

        if all_results:
            # æ’åºçµæœ
            def avg_score(result):
                train_sharpe = result['train_metrics'].get('sharpe_ratio', 0)
                val_sharpe = result['val_metrics'].get('sharpe_ratio', 0)
                train_sortino = result['train_metrics'].get('sortino_ratio', 0)
                val_sortino = result['val_metrics'].get('sortino_ratio', 0)
                train_trades = result['train_metrics'].get('trade_count', 0)
                val_trades = result['val_metrics'].get('trade_count', 0)

                train_score = (train_sharpe + train_sortino) * (1 + np.log1p(train_trades) / 10)
                val_score = (val_sharpe + val_sortino) * (1 + np.log1p(val_trades) / 10)

                if train_sharpe <= 0 or val_sharpe <= 0:
                    return float('-inf')

                return (train_score + val_score) / 2

            best_result = max(all_results, key=avg_score)
            best_params = best_result['params']

            # æœ€çµ‚é©—è­‰
            final_validation_metrics = self.evaluate_strategy(
                strategy_factory(best_params),
                self.validation_data
            )

            param_stability = self._calculate_param_stability(all_results, best_params)

            logger.info(f"ç¶²æ ¼æœç´¢å®Œæˆï¼Œæ‰¾åˆ°æœ€ä½³åƒæ•¸ï¼š{best_params}")
            logger.info(f"æœ€ä½³ Sharpe æ¯”ç‡: {final_validation_metrics.get('sharpe_ratio', 0):.4f}")

            self.optimization_stats['best_sharpe'] = final_validation_metrics.get('sharpe_ratio', 0)

            return {
                'best_params': best_params,
                'param_stability': param_stability,
                'train_metrics': best_result['train_metrics'],
                'cv_metrics': best_result['val_metrics'],
                'validation_metrics': final_validation_metrics,
                'method': 'Grid Search with Cross-Validation',
                'total_evaluated': len(all_results)
            }

        logger.warning("ç¶²æ ¼æœç´¢æœªæ‰¾åˆ°æœ‰æ•ˆåƒæ•¸çµ„åˆ")
        return {}

    def random_search(self, strategy_factory: Callable, param_grid: Dict, n_iter: int = 100) -> Dict:
        """
        éš¨æ©Ÿæœç´¢ï¼ŒåŒ…å«äº¤å‰é©—è­‰

        Args:
            strategy_factory: ç­–ç•¥å·¥å» å‡½æ•¸
            param_grid: åƒæ•¸ç¶²æ ¼
            n_iter: æ¯æŠ˜çš„è¿­ä»£æ¬¡æ•¸

        Returns:
            Dict: å„ªåŒ–çµæœ
        """
        self._validate_param_grid(param_grid)

        logger.info(f"é–‹å§‹éš¨æ©Ÿæœç´¢ï¼Œæ¯æŠ˜è¿­ä»£æ¬¡æ•¸ï¼š{n_iter}")

        n_splits = 5
        data_length = len(self.train_data)
        fold_size = data_length // n_splits

        all_results = []
        for fold in range(n_splits):
            logger.info(f"é–‹å§‹ç¬¬ {fold + 1} æŠ˜äº¤å‰é©—è­‰")

            val_start = fold * fold_size
            val_end = val_start + fold_size
            if fold == n_splits - 1:
                val_end = data_length

            train_data = pd.concat([
                self.train_data.iloc[:val_start],
                self.train_data.iloc[val_end:]
            ])
            val_data = self.train_data.iloc[val_start:val_end]

            # éš¨æ©Ÿæ¡æ¨£åƒæ•¸
            for _ in range(n_iter):
                try:
                    param_dict = {name: random.choice(values) for name, values in param_grid.items()}
                    strategy = strategy_factory(param_dict)
                    train_metrics = self.evaluate_strategy(strategy, train_data)

                    if train_metrics and 'sharpe_ratio' in train_metrics and train_metrics['trade_count'] > 0:
                        val_metrics = self.evaluate_strategy(strategy, val_data)
                        if val_metrics:
                            all_results.append({
                                'params': param_dict,
                                'train_metrics': train_metrics,
                                'val_metrics': val_metrics,
                                'fold': fold
                            })
                except Exception as e:
                    logger.debug(f"åƒæ•¸è©•ä¼°å¤±æ•—: {e}")

            del train_data, val_data
            gc.collect()

        if all_results:
            # æ’åºå’Œå ±å‘Š
            def avg_score(result):
                train_sharpe = result['train_metrics'].get('sharpe_ratio', 0)
                val_sharpe = result['val_metrics'].get('sharpe_ratio', 0)
                if train_sharpe <= 0 or val_sharpe <= 0:
                    return float('-inf')
                return (train_sharpe + val_sharpe) / 2

            best_result = max(all_results, key=avg_score)
            best_params = best_result['params']

            final_validation_metrics = self.evaluate_strategy(
                strategy_factory(best_params),
                self.validation_data
            )

            param_stability = self._calculate_param_stability(all_results, best_params)
            param_distribution = self._calculate_param_distribution(all_results)

            return {
                'best_params': best_params,
                'param_stability': param_stability,
                'param_distribution': param_distribution,
                'train_metrics': best_result['train_metrics'],
                'cv_metrics': best_result['val_metrics'],
                'validation_metrics': final_validation_metrics,
                'method': 'Random Search with Cross-Validation',
                'total_evaluated': len(all_results)
            }

        logger.warning("éš¨æ©Ÿæœç´¢æœªæ‰¾åˆ°æœ‰æ•ˆåƒæ•¸çµ„åˆ")
        return {}

    def brute_force(self, test_func: Callable, param_combinations: List,
                   max_processes: Optional[int] = None) -> List:
        """
        æš´åŠ›æœç´¢æ‰€æœ‰åƒæ•¸çµ„åˆ

        Args:
            test_func: æ¸¬è©¦å‡½æ•¸
            param_combinations: åƒæ•¸çµ„åˆåˆ—è¡¨
            max_processes: æœ€å¤§é€²ç¨‹æ•¸

        Returns:
            List: æœ‰æ•ˆçµæœåˆ—è¡¨
        """
        if max_processes is None or max_processes <= 0:
            max_processes = self.max_processes

        total_combinations = len(param_combinations)
        logger.info(f"é–‹å§‹æš´åŠ›æœç´¢ï¼Œå…± {total_combinations} çµ„åƒæ•¸çµ„åˆ")

        batch_size = min(1000, total_combinations)
        num_batches = (total_combinations + batch_size - 1) // batch_size

        all_results = []
        total_valid = 0
        total_processed = 0

        try:
            for batch_idx in range(num_batches):
                start_idx = batch_idx * batch_size
                end_idx = min(start_idx + batch_size, total_combinations)
                current_batch = param_combinations[start_idx:end_idx]

                logger.info(f"è™•ç†ç¬¬ {batch_idx + 1}/{num_batches} æ‰¹æ¬¡")

                with Pool(processes=max_processes) as pool:
                    batch_results = list(tqdm(
                        pool.imap(test_func, current_batch),
                        total=len(current_batch),
                        desc=f"æ‰¹æ¬¡ {batch_idx + 1} é€²åº¦"
                    ))

                valid_results = [r for r in batch_results if r]
                all_results.extend(valid_results)

                total_valid += len(valid_results)
                total_processed += len(current_batch)

                del batch_results, valid_results
                gc.collect()

        except Exception as e:
            logger.error(f"æš´åŠ›æœç´¢éç¨‹ä¸­ç™¼ç”ŸéŒ¯èª¤: {e}")
            return all_results

        logger.info(f"æš´åŠ›æœç´¢å®Œæˆï¼Œå…±æ‰¾åˆ° {len(all_results)} å€‹æœ‰æ•ˆçµæœ")
        return all_results

    def get_stats(self) -> Dict:
        """ç²å–å„ªåŒ–çµ±è¨ˆä¿¡æ¯"""
        return self.optimization_stats.copy()
