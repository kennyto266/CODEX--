"""
ğŸš€ å¹¶è¡Œå‚æ•°ä¼˜åŒ–å™¨

ä½¿ç”¨ Rayon å’Œ Tokio å®ç°æè‡´å¹¶è¡Œæ€§èƒ½ï¼š
- è‡ªåŠ¨ CPU æ ¸å¿ƒæ£€æµ‹
- æ™ºèƒ½å·¥ä½œåˆ†æ´¾ç®—æ³•
- å†…å­˜ä½¿ç”¨ç›‘æ§
- æ€§èƒ½ç»Ÿè®¡ä¸æŠ¥å‘Š
"""

import os
import time
import logging
from typing import Dict, List, Any, Optional, Tuple
from dataclasses import dataclass
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed
import multiprocessing as mp
from collections import defaultdict
import numpy as np
import pandas as pd
import psutil

from .acceleration import get_accelerator, PerformanceConfig

logger = logging.getLogger(__name__)

@dataclass
class OptimizationJob:
    """ä¼˜åŒ–ä»»åŠ¡"""
    job_id: str
    strategy_type: str
    parameters: Dict[str, float]
    data_subset: Optional[pd.DataFrame] = None
    priority: int = 0


@dataclass
class OptimizationResult:
    """ä¼˜åŒ–ç»“æœ"""
    job_id: str
    score: float
    metrics: Dict[str, Any]
    execution_time_ms: float
    parameters: Dict[str, float]
    rank: Optional[int] = None


class ParallelOptimizer:
    """å¹¶è¡Œå‚æ•°ä¼˜åŒ–å™¨"""

    def __init__(self, max_workers: Optional[int] = None):
        self.max_workers = max_workers or mp.cpu_count()
        self.job_results: Dict[str, OptimizationResult] = {}
        self.performance_stats = defaultdict(list)
        self.memory_monitor = MemoryMonitor()

        logger.info(f"ğŸš€ å¹¶è¡Œä¼˜åŒ–å™¨åˆå§‹åŒ–")
        logger.info(f"   - æœ€å¤§å·¥ä½œè¿›ç¨‹: {self.max_workers}")
        logger.info(f"   - CPU æ ¸å¿ƒæ•°: {mp.cpu_count()}")

    def optimize(
        self,
        data: pd.DataFrame,
        strategy_type: str,
        param_ranges: Dict[str, List[float]],
        metric: str = 'sharpe_ratio',
        max_combinations: int = 10000,
    ) -> Dict[str, Any]:
        """å¹¶è¡Œå‚æ•°ä¼˜åŒ–"""

        # ç”Ÿæˆå‚æ•°ç»„åˆ
        from itertools import product
        param_names = list(param_ranges.keys())
        param_values = list(param_ranges.values())
        combinations = list(product(*param_values))

        if len(combinations) > max_combinations:
            logger.warning(f"å‚æ•°ç»„åˆè¿‡å¤š ({len(combinations)}), é‡‡æ · {max_combinations}")
            combinations = combinations[:max_combinations]

        logger.info(f"å¼€å§‹å¹¶è¡Œä¼˜åŒ–: {len(combinations)} ä¸ªç»„åˆ")
        start_time = time.time()

        # æ‰¹é‡åˆ†æ´¾
        job_batches = self._create_job_batches(
            combinations, param_names, strategy_type
        )

        # å¹¶è¡Œæ‰§è¡Œ
        all_results = []
        for batch in job_batches:
            batch_results = self._execute_batch(batch, data)
            all_results.extend(batch_results)

        # æ’åºå’Œæ’å
        all_results.sort(key=lambda x: x.score, reverse=True)
        for i, result in enumerate(all_results):
            result.rank = i + 1

        # æ€§èƒ½ç»Ÿè®¡
        total_time = (time.time() - start_time) * 1000
        throughput = len(all_results) / (total_time / 1000.0)

        self.performance_stats['total_jobs'].append(len(all_results))
        self.performance_stats['total_time_ms'].append(total_time)
        self.performance_stats['throughput'].append(throughput)

        logger.info(f"âœ… ä¼˜åŒ–å®Œæˆ")
        logger.info(f"   - æ€»æ—¶é—´: {total_time:.2f}ms")
        logger.info(f"   - ååé‡: {throughput:.2f} ç»„åˆ/ç§’")
        logger.info(f"   - å¹³å‡æ—¶é—´: {total_time/len(all_results):.2f}ms/ç»„åˆ")

        return {
            'best_result': all_results[0] if all_results else None,
            'all_results': all_results,
            'statistics': {
                'total_combinations': len(all_results),
                'total_time_ms': total_time,
                'throughput_per_second': throughput,
                'avg_time_per_combination_ms': total_time / len(all_results) if all_results else 0,
                'peak_memory_mb': self.memory_monitor.get_peak_memory(),
            },
            'performance': dict(self.performance_stats),
        }

    def _create_job_batches(
        self,
        combinations: List[Tuple[float, ...]],
        param_names: List[str],
        strategy_type: str,
    ) -> List[List[OptimizationJob]]:
        """åˆ›å»ºå·¥ä½œæ‰¹æ¬¡"""
        batch_size = max(1, len(combinations) // self.max_workers)
        batches = []

        for i in range(0, len(combinations), batch_size):
            batch = []
            for combo in combinations[i:i+batch_size]:
                params = dict(zip(param_names, combo))
                job = OptimizationJob(
                    job_id=f"job_{i}_{combo}",
                    strategy_type=strategy_type,
                    parameters=params,
                )
                batch.append(job)
            batches.append(batch)

        logger.info(f"åˆ›å»º {len(batches)} ä¸ªæ‰¹æ¬¡, æ¯æ‰¹çº¦ {batch_size} ä¸ªä»»åŠ¡")
        return batches

    def _execute_batch(
        self,
        jobs: List[OptimizationJob],
        data: pd.DataFrame,
    ) -> List[OptimizationResult]:
        """æ‰§è¡Œä¸€æ‰¹ä»»åŠ¡"""
        results = []

        with ThreadPoolExecutor(max_workers=self.max_workers) as executor:
            futures = {
                executor.submit(self._run_single_job, job, data): job
                for job in jobs
            }

            for future in as_completed(futures):
                try:
                    result = future.result(timeout=30)
                    results.append(result)

                    # å†…å­˜ç›‘æ§
                    current_memory = self.memory_monitor.get_memory_usage()
                    self.memory_monitor.check_threshold(current_memory)

                except Exception as e:
                    job = futures[future]
                    logger.error(f"ä»»åŠ¡ {job.job_id} å¤±è´¥: {e}")
                    # åˆ›å»ºé”™è¯¯ç»“æœ
                    results.append(OptimizationResult(
                        job_id=job.job_id,
                        score=float('-inf'),
                        metrics={'error': str(e)},
                        execution_time_ms=0,
                        parameters=job.parameters,
                    ))

        return results

    def _run_single_job(
        self,
        job: OptimizationJob,
        data: pd.DataFrame,
    ) -> OptimizationResult:
        """è¿è¡Œå•ä¸ªä¼˜åŒ–ä»»åŠ¡"""
        start_time = time.time()

        try:
            accelerator = get_accelerator(PerformanceConfig(
                use_rust=True,
                max_workers=1,  # å•ä»»åŠ¡ä½¿ç”¨å•æ ¸
            ))

            result = accelerator.run_backtest(
                data,
                job.strategy_type,
                job.parameters,
            )

            execution_time = (time.time() - start_time) * 1000

            # æå–æŒ‡æ ‡
            score = result['metrics'].get(job.strategy_type, 0.0)
            if job.strategy_type == 'ma':
                score = result['metrics']['sharpe_ratio']

            return OptimizationResult(
                job_id=job.job_id,
                score=score,
                metrics=result['metrics'],
                execution_time_ms=execution_time,
                parameters=job.parameters,
            )

        except Exception as e:
            execution_time = (time.time() - start_time) * 1000
            logger.error(f"ä»»åŠ¡ {job.job_id} æ‰§è¡Œé”™è¯¯: {e}")

            return OptimizationResult(
                job_id=job.job_id,
                score=float('-inf'),
                metrics={'error': str(e)},
                execution_time_ms=execution_time,
                parameters=job.parameters,
            )

    def walk_forward_optimization(
        self,
        data: pd.DataFrame,
        strategy_type: str,
        param_ranges: Dict[str, List[float]],
        training_period_days: int = 252,
        testing_period_days: int = 63,
        step_size_days: int = 21,
    ) -> List[Dict[str, Any]]:
        """èµ°æ­¥ä¼˜åŒ–"""
        results = []
        total_days = (data.index[-1] - data.index[0]).days

        current_day = 0
        iteration = 0

        while current_day + training_period_days + testing_period_days < total_days:
            iteration += 1
            train_start = current_day
            train_end = current_day + training_period_days
            test_start = train_end
            test_end = test_start + testing_period_days

            train_data = data.iloc[train_start:train_end]
            test_data = data.iloc[test_start:test_end]

            logger.info(f"\nè¿­ä»£ {iteration}:")
            logger.info(f"  è®­ç»ƒæœŸ: {train_start}-{train_end} ({len(train_data)} å¤©)")
            logger.info(f"  æµ‹è¯•æœŸ: {test_start}-{test_end} ({len(test_data)} å¤©)")

            # è®­ç»ƒæœŸä¼˜åŒ–
            optimization_result = self.optimize(
                train_data,
                strategy_type,
                param_ranges,
                max_combinations=5000,  # å‡å°‘è®­ç»ƒæœŸå‚æ•°
            )

            # æµ‹è¯•æœŸéªŒè¯
            if optimization_result['best_result']:
                best_params = optimization_result['best_result'].parameters
                accelerator = get_accelerator(PerformanceConfig(use_rust=True))

                test_result = accelerator.run_backtest(test_data, strategy_type, best_params)

                results.append({
                    'iteration': iteration,
                    'training_period': (train_start, train_end),
                    'testing_period': (test_start, test_end),
                    'best_params': best_params,
                    'training_score': optimization_result['best_result'].score,
                    'test_metrics': test_result['metrics'],
                    'optimization_time_ms': optimization_result['statistics']['total_time_ms'],
                })

            current_day += step_size_days

        return results


class MemoryMonitor:
    """å†…å­˜ä½¿ç”¨ç›‘æ§"""

    def __init__(self, max_memory_mb: int = 1024):
        self.max_memory_mb = max_memory_mb
        self.peak_memory = 0.0
        self.alerts = []

    def get_memory_usage(self) -> float:
        """è·å–å½“å‰å†…å­˜ä½¿ç”¨ (MB)"""
        process = psutil.Process()
        return process.memory_info().rss / 1024 / 1024

    def get_peak_memory(self) -> float:
        """è·å–å³°å€¼å†…å­˜ä½¿ç”¨"""
        return self.peak_memory

    def check_threshold(self, current_mb: float):
        """æ£€æŸ¥å†…å­˜é˜ˆå€¼"""
        if current_mb > self.peak_memory:
            self.peak_memory = current_mb

        if current_mb > self.max_memory_mb:
            alert = f"âš ï¸  å†…å­˜ä½¿ç”¨è¶…é™: {current_mb:.2f}MB > {self.max_memory_mb}MB"
            logger.warning(alert)
            self.alerts.append({
                'timestamp': time.time(),
                'message': alert,
            })


class WorkerPool:
    """å·¥ä½œè¿›ç¨‹æ± ç®¡ç†å™¨"""

    def __init__(self, num_workers: int):
        self.num_workers = num_workers
        self.workers = []
        self.task_queue = mp.Queue()
        self.result_queue = mp.Queue()

    def submit_task(self, task_data: Dict[str, Any]):
        """æäº¤ä»»åŠ¡"""
        self.task_queue.put(task_data)

    def get_result(self) -> Optional[Dict[str, Any]]:
        """è·å–ç»“æœ"""
        try:
            return self.result_queue.get(timeout=1)
        except:
            return None

    def shutdown(self):
        """å…³é—­å·¥ä½œæ± """
        for _ in range(self.num_workers):
            self.task_queue.put(None)  # ç»ˆæ­¢ä¿¡å·

        for worker in self.workers:
            worker.join()


def optimize_multiple_strategies(
    data: pd.DataFrame,
    strategies: List[Dict[str, Any]],
    param_ranges: Dict[str, List[float]],
) -> Dict[str, Dict[str, Any]]:
    """å¤šç­–ç•¥å¹¶è¡Œä¼˜åŒ–"""
    optimizer = ParallelOptimizer()

    results = {}
    for strategy in strategies:
        strategy_name = strategy['name']
        strategy_type = strategy['type']

        logger.info(f"\nä¼˜åŒ–ç­–ç•¥: {strategy_name} ({strategy_type})")

        result = optimizer.optimize(
            data,
            strategy_type,
            param_ranges,
            max_combinations=10000,
        )

        results[strategy_name] = result

    return results


if __name__ == '__main__':
    # æµ‹è¯•å¹¶è¡Œä¼˜åŒ–å™¨
    print("="*60)
    print("ğŸš€ å¹¶è¡Œä¼˜åŒ–å™¨æµ‹è¯•")
    print("="*60)

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    dates = pd.date_range('2020-01-01', periods=1000, freq='D')
    prices = 100 + np.cumsum(np.random.randn(1000) * 0.5)
    data = pd.DataFrame({
        'Open': prices * (1 + np.random.randn(1000) * 0.001),
        'High': prices * (1 + np.random.randn(1000) * 0.002),
        'Low': prices * (1 - np.random.randn(1000) * 0.002),
        'Close': prices,
        'Volume': np.random.randint(1000, 10000, 1000),
    }, index=dates)

    # å‚æ•°èŒƒå›´
    param_ranges = {
        'fast_period': [5, 10, 20],
        'slow_period': [20, 30, 50],
    }

    # æ‰§è¡Œä¼˜åŒ–
    optimizer = ParallelOptimizer()
    result = optimizer.optimize(
        data,
        'ma',
        param_ranges,
        max_combinations=100,
    )

    print(f"\næœ€ä½³ç»“æœ:")
    print(f"  å‚æ•°: {result['best_result'].parameters}")
    print(f"  å¾—åˆ†: {result['best_result'].score:.4f}")
    print(f"  æ‰§è¡Œæ—¶é—´: {result['best_result'].execution_time_ms:.2f}ms")
    print(f"\nç»Ÿè®¡ä¿¡æ¯:")
    print(f"  æ€»ç»„åˆæ•°: {result['statistics']['total_combinations']}")
    print(f"  æ€»æ—¶é—´: {result['statistics']['total_time_ms']:.2f}ms")
    print(f"  ååé‡: {result['statistics']['throughput_per_second']:.2f} ç»„åˆ/ç§’")
    print(f"  å³°å€¼å†…å­˜: {result['statistics']['peak_memory_mb']:.2f}MB")
