import dask.dataframe as dd
import pandas as pd
from typing import Dict, List, Optional, Any
import logging
from pathlib import Path
import os

logger = logging.getLogger('quant_system')

try:
    import dask
    DASK_AVAILABLE = True
except ImportError:
    logger.warning("Dask not available, big data features disabled")
    DASK_AVAILABLE = False

class BigDataProcessor:
    """大数据处理器"""

    def __init__(self, chunk_size: str = '100MB'):
        if not DASK_AVAILABLE:
            raise ImportError("Dask required for big data processing")

        self.chunk_size = chunk_size

    def load_large_csv(self, file_path: str, **kwargs) -> dd.DataFrame:
        """加载大CSV文件"""
        try:
            # 使用Dask加载大数据文件
            df = dd.read_csv(file_path, blocksize=self.chunk_size, **kwargs)
            logger.info(f"Loaded large CSV: {file_path} with {len(df)} partitions")
            return df
        except Exception as e:
            logger.error(f"Failed to load large CSV: {e}")
            raise

    def process_historical_data(self, data_path: str, symbol: str = None) -> dd.DataFrame:
        """处理历史数据"""
        try:
            # 假设数据按日期分区存储
            pattern = f"{data_path}/historical_data_*.csv"
            if symbol:
                pattern = f"{data_path}/historical_data_{symbol}_*.csv"

            # 读取所有分区文件
            df = dd.read_csv(pattern, blocksize=self.chunk_size)

            # 添加基本处理
            df['date'] = dd.to_datetime(df['date'])
            df = df.set_index('date').sort_index()

            logger.info(f"Processed historical data: {len(df)} partitions")
            return df

        except Exception as e:
            logger.error(f"Failed to process historical data: {e}")
            raise

    def calculate_technical_indicators_batch(self, df: dd.DataFrame) -> dd.DataFrame:
        """批量计算技术指标"""
        try:
            # SMA
            df['SMA_5'] = df['close'].rolling(window=5).mean()
            df['SMA_20'] = df['close'].rolling(window=20).mean()

            # EMA
            df['EMA_5'] = df['close'].ewm(span=5).mean()
            df['EMA_20'] = df['close'].ewm(span=20).mean()

            # RSI
            delta = df['close'].diff()
            gain = delta.where(delta > 0, 0)
            loss = -delta.where(delta < 0, 0)
            avg_gain = gain.rolling(window=14).mean()
            avg_loss = loss.rolling(window=14).mean()
            rs = avg_gain / avg_loss
            df['RSI'] = 100 - (100 / (1 + rs))

            # MACD
            exp1 = df['close'].ewm(span=12).mean()
            exp2 = df['close'].ewm(span=26).mean()
            df['MACD'] = exp1 - exp2
            df['MACD_signal'] = df['MACD'].ewm(span=9).mean()

            logger.info("Calculated technical indicators for large dataset")
            return df

        except Exception as e:
            logger.error(f"Failed to calculate indicators: {e}")
            raise

    def parallel_backtest(self, df: dd.DataFrame, strategies: List[Dict]) -> dd.DataFrame:
        """并行回测多个策略"""
        try:
            results = []

            for strategy in strategies:
                strategy_name = strategy['name']
                params = strategy.get('params', {})

                # 为每个策略创建计算函数
                def apply_strategy(partition, strategy_name, params):
                    # 这里可以实现具体的策略逻辑
                    # 简化版：基于移动平均线交叉
                    partition = partition.copy()
                    partition[f'{strategy_name}_signal'] = 0

                    # 生成买卖信号
                    sma_short = partition['close'].rolling(window=params.get('short_window', 5)).mean()
                    sma_long = partition['close'].rolling(window=params.get('long_window', 20)).mean()

                    # 买入信号：短期上穿长期
                    buy_signal = (sma_short > sma_long) & (sma_short.shift(1) <= sma_long.shift(1))
                    # 卖出信号：短期下穿长期
                    sell_signal = (sma_short < sma_long) & (sma_short.shift(1) >= sma_long.shift(1))

                    partition.loc[buy_signal, f'{strategy_name}_signal'] = 1
                    partition.loc[sell_signal, f'{strategy_name}_signal'] = -1

                    return partition

                # 应用策略到每个分区
                result_df = df.map_partitions(apply_strategy, strategy_name, params)
                results.append(result_df)

            # 合并所有策略结果
            final_df = dd.concat(results, axis=1) if results else df

            logger.info(f"Completed parallel backtest for {len(strategies)} strategies")
            return final_df

        except Exception as e:
            logger.error(f"Failed parallel backtest: {e}")
            raise

    def optimize_strategy_batch(self, df: dd.DataFrame, strategy_func, param_grid: Dict[str, List]) -> Dict[str, Any]:
        """批量策略优化"""
        try:
            from itertools import product

            # 生成所有参数组合
            param_combinations = list(product(*param_grid.values()))
            param_names = list(param_grid.keys())

            best_result = None
            best_params = None
            best_score = float('-inf')

            # 并行测试每个参数组合
            tasks = []
            for params in param_combinations:
                param_dict = dict(zip(param_names, params))
                # 这里可以创建异步任务来并行优化
                # 简化版：串行处理
                result = self._evaluate_strategy_params(df, strategy_func, param_dict)

                if result['score'] > best_score:
                    best_score = result['score']
                    best_params = param_dict
                    best_result = result

            logger.info(f"Strategy optimization completed. Best params: {best_params}")
            return {
                'best_params': best_params,
                'best_score': best_score,
                'results': best_result
            }

        except Exception as e:
            logger.error(f"Strategy optimization failed: {e}")
            raise

    def _evaluate_strategy_params(self, df: dd.DataFrame, strategy_func, params: Dict) -> Dict[str, Any]:
        """评估策略参数"""
        try:
            # 应用策略参数
            result_df = df.map_partitions(strategy_func, params)

            # 计算绩效指标
            returns = result_df['strategy_returns'].sum().compute()
            sharpe_ratio = (returns.mean() / returns.std()) * (252 ** 0.5)  # 年化夏普比率
            max_drawdown = self._calculate_max_drawdown(result_df['cumulative_returns'].compute())

            return {
                'score': sharpe_ratio,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'total_return': returns.sum()
            }

        except Exception as e:
            logger.error(f"Strategy parameter evaluation failed: {e}")
            return {'score': float('-inf')}

    def _calculate_max_drawdown(self, returns: pd.Series) -> float:
        """计算最大回撤"""
        cumulative = (1 + returns).cumprod()
        running_max = cumulative.expanding().max()
        drawdown = (cumulative - running_max) / running_max
        return drawdown.min()

    def save_processed_data(self, df: dd.DataFrame, output_path: str, format: str = 'parquet'):
        """保存处理后的数据"""
        try:
            if format == 'parquet':
                df.to_parquet(output_path)
            elif format == 'csv':
                df.to_csv(output_path, single_file=True)
            else:
                raise ValueError(f"Unsupported format: {format}")

            logger.info(f"Saved processed data to {output_path} in {format} format")
            return output_path

        except Exception as e:
            logger.error(f"Failed to save processed data: {e}")
            raise

    def get_data_summary(self, df: dd.DataFrame) -> Dict[str, Any]:
        """获取数据摘要"""
        try:
            summary = {
                'total_rows': len(df),
                'partitions': df.npartitions,
                'columns': list(df.columns),
                'dtypes': df.dtypes.to_dict(),
                'memory_usage': df.memory_usage(deep=True).sum().compute()
            }

            # 计算基本统计（如果数据量不大）
            if len(df) < 1000000:  # 1M行以内
                numeric_cols = df.select_dtypes(include=[float, int]).columns
                if len(numeric_cols) > 0:
                    summary['stats'] = df[numeric_cols].describe().compute().to_dict()

            return summary

        except Exception as e:
            logger.error(f"Failed to get data summary: {e}")
            return {}

# 全局实例
if DASK_AVAILABLE:
    big_data_processor = BigDataProcessor()
else:
    big_data_processor = None