#!/usr/bin/env python3
"""
HKMA HIBORæ•°æ®é€‚é…å™¨ - æ¼”ç¤ºè„šæœ¬
å±•ç¤ºå¦‚ä½•ä½¿ç”¨5ä¸ªHKMAæ¨¡å—è¿›è¡Œå®Œæ•´çš„HIBORæ•°æ®å¤„ç†
"""

import asyncio
import json
import logging
from datetime import date, datetime, timedelta
from typing import Dict, Any

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# å¯¼å…¥HKMAæ¨¡å—
from src.data_adapters.hkma_hibor import HKMAHibiorAdapter
from src.data_adapters.hibor_validator import HibiorDataValidator
from src.data_adapters.hkma_scheduler import TaskScheduler, hibor_update_handler
from src.data_adapters.hkma_error_handler import HKMAErrorHandler
from src.data_adapters.hkma_monitor import HKMAMonitor


class HKMASystemDemo:
    """HKMA HIBORç³»ç»Ÿæ¼”ç¤ºç±»"""

    def __init__(self):
        self.adapter = None
        self.validator = None
        self.scheduler = None
        self.error_handler = None
        self.monitor = None

    async def initialize(self):
        """åˆå§‹åŒ–æ‰€æœ‰ç»„ä»¶"""
        logger.info("åˆå§‹åŒ–HKMA HIBORç³»ç»Ÿ...")

        # åˆ›å»ºé€‚é…å™¨
        self.adapter = HKMAHibiorAdapter(config={
            'timeout': 30,
            'max_retries': 3
        })

        # åˆ›å»ºéªŒè¯å™¨
        self.validator = HibiorDataValidator(config={
            'strict_mode': False,
            'check_trends': True
        })

        # åˆ›å»ºè°ƒåº¦å™¨
        self.scheduler = TaskScheduler(config={
            'scheduler_interval': 60,
            'max_concurrent_tasks': 3
        })
        self.scheduler.register_handler("daily_update", hibor_update_handler)
        self.scheduler.register_handler("historical_update", hibor_update_handler)

        # åˆ›å»ºé”™è¯¯å¤„ç†å™¨
        self.error_handler = HKMAErrorHandler(config={
            'degraded_mode_threshold': 5
        })

        # åˆ›å»ºç›‘æ§å™¨
        self.monitor = HKMAMonitor(config={
            'monitor_interval': 300,
            'auto_monitoring': True
        })

        # æ·»åŠ å‘Šè­¦å›è°ƒ
        self.monitor.add_alert_callback(self._alert_callback)

        logger.info("æ‰€æœ‰ç»„ä»¶åˆå§‹åŒ–å®Œæˆ")

    async def _alert_callback(self, alert):
        """å‘Šè­¦å›è°ƒå‡½æ•°"""
        logger.warning(
            f"ğŸš¨ æ•°æ®è´¨é‡å‘Šè­¦ [{alert.level.value.upper()}]: "
            f"{alert.message} (å€¼: {alert.value:.1f}, é˜ˆå€¼: {alert.threshold:.1f})"
        )

    async def demo_fetch_latest_data(self):
        """æ¼”ç¤ºï¼šè·å–æœ€æ–°HIBORæ•°æ®"""
        logger.info("\n" + "=" * 60)
        logger.info("æ¼”ç¤º1: è·å–æœ€æ–°HIBORæ•°æ®")
        logger.info("=" * 60)

        try:
            async with self.adapter as adapter:
                # å°è¯•è·å–æœ€æ–°HIBORæ•°æ®
                result = await self.error_handler.execute_with_retry(
                    adapter.fetch_latest_hibor,
                    max_retries=3,
                    context={'operation': 'fetch_latest'}
                )

                if result:
                    logger.info("âœ… æˆåŠŸè·å–æœ€æ–°HIBORæ•°æ®:")
                    logger.info(f"  æ—¥æœŸ: {result.get('date')}")
                    if 'data' in result:
                        for key, value in result['data'].items():
                            if value is not None:
                                logger.info(f"  {key}: {value}")
                    return result
                else:
                    logger.warning("âš ï¸ æœªèƒ½è·å–HIBORæ•°æ®ï¼ˆå¯èƒ½æ˜¯ç½‘ç»œé—®é¢˜ï¼‰")
                    return None

        except Exception as e:
            logger.error(f"âŒ è·å–æ•°æ®å¤±è´¥: {e}")
            return None

    async def demo_validate_data(self, data: Dict[str, Any]):
        """æ¼”ç¤ºï¼šéªŒè¯æ•°æ®è´¨é‡"""
        logger.info("\n" + "=" * 60)
        logger.info("æ¼”ç¤º2: éªŒè¯æ•°æ®è´¨é‡")
        logger.info("=" * 60)

        if not data:
            logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®å¯éªŒè¯")
            return None

        try:
            # éªŒè¯æ•°æ®
            result = self.validator.validate_hibor_data(data, term='1m')

            logger.info(f"éªŒè¯ç»“æœ:")
            logger.info(f"  æ€»ä½“çŠ¶æ€: {'âœ… æœ‰æ•ˆ' if result.is_valid else 'âŒ æ— æ•ˆ'}")
            logger.info(f"  æœ‰æ•ˆè®°å½•: {result.valid_count}")
            logger.info(f"  æ— æ•ˆè®°å½•: {result.invalid_count}")
            logger.info(f"  è­¦å‘Šæ•°é‡: {result.warning_count}")
            logger.info(f"  æ€»é—®é¢˜æ•°: {result.total_issues}")

            if result.issues:
                logger.info("\né—®é¢˜è¯¦æƒ…:")
                for issue in result.issues[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ªé—®é¢˜
                    icon = {
                        'info': 'â„¹ï¸',
                        'warning': 'âš ï¸',
                        'error': 'âŒ',
                        'critical': 'ğŸš¨'
                    }.get(issue.severity.value, 'â€¢')

                    logger.info(
                        f"  {icon} [{issue.severity.value.upper()}] "
                        f"{issue.field}: {issue.message}"
                    )

                    if issue.value is not None:
                        logger.info(f"    å®é™…å€¼: {issue.value}")
                    if issue.expected is not None:
                        logger.info(f"    æœŸæœ›å€¼: {issue.expected}")

            return result

        except Exception as e:
            logger.error(f"âŒ æ•°æ®éªŒè¯å¤±è´¥: {e}")
            return None

    async def demo_monitor_data(self, data: Dict[str, Any]):
        """æ¼”ç¤ºï¼šç›‘æ§æ•°æ®è´¨é‡"""
        logger.info("\n" + "=" * 60)
        logger.info("æ¼”ç¤º3: ç›‘æ§æ•°æ®è´¨é‡")
        logger.info("=" * 60)

        if not data:
            logger.warning("âš ï¸ æ²¡æœ‰æ•°æ®å¯ç›‘æ§")
            return None

        try:
            # æ·»åŠ æ•°æ®ç‚¹åˆ°ç›‘æ§å™¨
            await self.monitor.add_data_point(data)

            # è®¡ç®—è´¨é‡æŒ‡æ ‡
            metrics = await self.monitor.calculate_quality_metrics()

            logger.info(f"æ•°æ®è´¨é‡æŒ‡æ ‡:")
            logger.info(f"  æ€»ä½“è¯„åˆ†: {metrics.overall_score:.1f}/100")
            logger.info(f"  è´¨é‡ç­‰çº§: {metrics.get_quality_level().value.upper()}")

            detailed_metrics = {
                'æ–°é²œåº¦': metrics.freshness_score,
                'å®Œæ•´æ€§': metrics.completeness_score,
                'å‡†ç¡®æ€§': metrics.accuracy_score,
                'ä¸€è‡´æ€§': metrics.consistency_score,
                'è¶‹åŠ¿': metrics.trend_score,
                'æ³¢åŠ¨æ€§': metrics.volatility_score,
                'å¯ç”¨æ€§': metrics.availability_score
            }

            logger.info("\nè¯¦ç»†æŒ‡æ ‡:")
            for name, score in detailed_metrics.items():
                bar = 'â–ˆ' * int(score / 10) + 'â–‘' * (10 - int(score / 10))
                logger.info(f"  {name}: [{bar}] {score:.1f}")

            logger.info(f"\nç»Ÿè®¡ä¿¡æ¯:")
            logger.info(f"  æ•°æ®ç‚¹æ•°: {metrics.data_points}")
            logger.info(f"  ç¼ºå¤±å€¼: {metrics.missing_count}")
            logger.info(f"  å¼‚å¸¸å€¼: {metrics.anomalies_count}")

            # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            report = await self.monitor.get_quality_report()
            logger.info(f"\nå»ºè®®:")
            for rec in report['recommendations']:
                logger.info(f"  â€¢ {rec}")

            return metrics

        except Exception as e:
            logger.error(f"âŒ æ•°æ®ç›‘æ§å¤±è´¥: {e}")
            return None

    async def demo_schedule_tasks(self):
        """æ¼”ç¤ºï¼šè°ƒåº¦å®šæ—¶ä»»åŠ¡"""
        logger.info("\n" + "=" * 60)
        logger.info("æ¼”ç¤º4: è°ƒåº¦å®šæ—¶ä»»åŠ¡")
        logger.info("=" * 60)

        try:
            # è®¡åˆ’æ¯æ—¥æ›´æ–°ä»»åŠ¡
            task_id1 = await self.scheduler.schedule_daily_update(
                "hibor_morning",
                "08:00",
                {'frequency': 'daily'},
                priority=2  # HIGH
            )
            logger.info(f"âœ… å·²è®¡åˆ’æ¯æ—¥æ›´æ–°ä»»åŠ¡: {task_id1}")

            # è®¡åˆ’å†å²æ•°æ®æ›´æ–°ä»»åŠ¡
            start_date = date.today() - timedelta(days=30)
            end_date = date.today()
            task_id2 = await self.scheduler.schedule_historical_data_update(
                start_date,
                end_date,
                priority=1  # NORMAL
            )
            logger.info(f"âœ… å·²è®¡åˆ’å†å²æ•°æ®æ›´æ–°ä»»åŠ¡: {task_id2}")

            # æŸ¥çœ‹ä»»åŠ¡åˆ—è¡¨
            tasks = await self.scheduler.list_tasks(limit=10)
            logger.info(f"\nå½“å‰ä»»åŠ¡åˆ—è¡¨ ({len(tasks)} ä¸ª):")
            for task in tasks:
                status_icon = {
                    'pending': 'â³',
                    'running': 'ğŸ”„',
                    'completed': 'âœ…',
                    'failed': 'âŒ',
                    'cancelled': 'ğŸš«'
                }.get(task['status'], 'â€¢')

                priority_icon = {
                    1: 'ğŸ”µ',
                    2: 'ğŸŸ¡',
                    3: 'ğŸŸ ',
                    4: 'ğŸ”´'
                }.get(task['priority'], 'â€¢')

                logger.info(
                    f"  {status_icon} {priority_icon} {task['name']} "
                    f"({task['type']}) - {task['status']}"
                )

            # è·å–ç»Ÿè®¡ä¿¡æ¯
            stats = self.scheduler.get_statistics()
            logger.info(f"\nè°ƒåº¦å™¨ç»Ÿè®¡:")
            logger.info(f"  æ€»ä»»åŠ¡: {stats['total_tasks']}")
            logger.info(f"  å¾…æ‰§è¡Œ: {stats['pending_tasks']}")
            logger.info(f"  è¿è¡Œä¸­: {stats['running_tasks']}")
            logger.info(f"  å·²å®Œæˆ: {stats['completed_tasks']}")
            logger.info(f"  å·²å¤±è´¥: {stats['failed_tasks']}")

            return [task_id1, task_id2]

        except Exception as e:
            logger.error(f"âŒ ä»»åŠ¡è°ƒåº¦å¤±è´¥: {e}")
            return []

    async def demo_error_handling(self):
        """æ¼”ç¤ºï¼šé”™è¯¯å¤„ç†å’Œé‡è¯•"""
        logger.info("\n" + "=" * 60)
        logger.info("æ¼”ç¤º5: é”™è¯¯å¤„ç†å’Œé‡è¯•")
        logger.info("=" * 60)

        # æ¨¡æ‹Ÿä¸€ä¸ªä¼šå¤±è´¥çš„å‡½æ•°
        attempt_count = 0

        async def flaky_function():
            nonlocal attempt_count
            attempt_count += 1
            if attempt_count < 3:
                raise ConnectionError("æ¨¡æ‹Ÿç½‘ç»œé”™è¯¯")
            return f"ç¬¬{attempt_count}æ¬¡å°è¯•æˆåŠŸ"

        try:
            logger.info("æµ‹è¯•é‡è¯•æœºåˆ¶...")
            result = await self.error_handler.execute_with_retry(
                flaky_function,
                max_retries=5,
                context={'operation': 'test_retry'}
            )
            logger.info(f"âœ… é‡è¯•æˆåŠŸ: {result}")

        except Exception as e:
            logger.error(f"âŒ é‡è¯•å¤±è´¥: {e}")

        # è·å–é”™è¯¯ç»Ÿè®¡
        summary = self.error_handler.get_error_summary()
        logger.info(f"\né”™è¯¯ç»Ÿè®¡:")
        logger.info(f"  æ€»é”™è¯¯æ•°: {summary['total_errors']}")
        logger.info(f"  é”™è¯¯ç±»å‹: {summary['unique_error_types']}")

        if summary['most_common_error']:
            logger.info(
                f"  æœ€å¸¸è§é”™è¯¯: {summary['most_common_error']['type']} "
                f"({summary['most_common_error']['count']} æ¬¡)"
            )

        return summary

    async def demo_full_workflow(self):
        """æ¼”ç¤ºï¼šå®Œæ•´å·¥ä½œæµç¨‹"""
        logger.info("\n" + "=" * 60)
        logger.info("æ¼”ç¤º6: å®Œæ•´å·¥ä½œæµç¨‹")
        logger.info("=" * 60)

        try:
            # 1. è·å–æ•°æ®
            logger.info("æ­¥éª¤1: è·å–æ•°æ®...")
            data = await self.demo_fetch_latest_data()

            # 2. éªŒè¯æ•°æ®
            logger.info("æ­¥éª¤2: éªŒè¯æ•°æ®...")
            validation_result = await self.demo_validate_data(data)

            # 3. ç›‘æ§æ•°æ®
            logger.info("æ­¥éª¤3: ç›‘æ§æ•°æ®...")
            metrics = await self.demo_monitor_data(data)

            # 4. ç”ŸæˆæŠ¥å‘Š
            logger.info("æ­¥éª¤4: ç”ŸæˆæŠ¥å‘Š...")
            report = await self.monitor.get_quality_report()
            logger.info(f"\nğŸ“Š æœ€ç»ˆè´¨é‡æŠ¥å‘Š:")
            logger.info(f"  æ€»ä½“è¯„åˆ†: {report['overall_score']:.1f}/100")
            logger.info(f"  è´¨é‡ç­‰çº§: {report['quality_level'].upper()}")
            logger.info(f"  è´¨é‡è¶‹åŠ¿: {report['quality_trend']}")
            logger.info(f"  æ´»è·ƒå‘Šè­¦: {report['active_alerts']}")

            # 5. æ£€æŸ¥è´¨é‡è¶‹åŠ¿
            logger.info("æ­¥éª¤5: åˆ†æè¶‹åŠ¿...")
            trend = self.monitor.get_quality_trend(days=7)
            logger.info(f"\nğŸ“ˆ è´¨é‡è¶‹åŠ¿ (7å¤©):")
            logger.info(f"  è¶‹åŠ¿æ–¹å‘: {trend['trend']}")
            logger.info(f"  å¹³å‡è¯„åˆ†: {trend['avg_score']:.1f}")
            logger.info(f"  æœ€é«˜è¯„åˆ†: {trend['max_score']:.1f}")
            logger.info(f"  æœ€ä½è¯„åˆ†: {trend['min_score']:.1f}")

            logger.info("\nâœ… å®Œæ•´å·¥ä½œæµç¨‹æ¼”ç¤ºå®Œæˆ")

        except Exception as e:
            logger.error(f"âŒ å·¥ä½œæµç¨‹å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()

    async def run_all_demos(self):
        """è¿è¡Œæ‰€æœ‰æ¼”ç¤º"""
        logger.info("\n" + "=" * 60)
        logger.info("ğŸš€ HKMA HIBORæ•°æ®é€‚é…å™¨ - å®Œæ•´æ¼”ç¤º")
        logger.info("=" * 60)

        # åˆå§‹åŒ–
        await self.initialize()

        # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½
        await self.demo_schedule_tasks()
        await self.demo_error_handling()
        await self.demo_full_workflow()

        logger.info("\n" + "=" * 60)
        logger.info("âœ¨ æ‰€æœ‰æ¼”ç¤ºå®Œæˆ")
        logger.info("=" * 60)


async def main():
    """ä¸»å‡½æ•°"""
    demo = HKMASystemDemo()
    await demo.run_all_demos()


if __name__ == "__main__":
    # è¿è¡Œæ¼”ç¤º
    try:
        asyncio.run(main())
    except KeyboardInterrupt:
        logger.info("\næ¼”ç¤ºè¢«ç”¨æˆ·ä¸­æ–­")
    except Exception as e:
        logger.error(f"\næ¼”ç¤ºè¿è¡Œå¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
