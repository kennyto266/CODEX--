"""
Sprint 4 é›†æˆç¤ºä¾‹
å±•ç¤ºåç¨‹æ±  + Protocol Buffers + èƒŒå‹æ§åˆ¶çš„å®Œæ•´ä½¿ç”¨
"""

import asyncio
import logging
import time
from typing import List, Dict, Any
import json

from src.utils.performance_utils import (
    initialize_performance_system,
    execute_with_pool,
    rate_limited_execute,
    serialize_and_send,
    receive_and_deserialize,
    batch_process,
    high_throughput_processor,
    get_performance_stats,
    benchmark_throughput,
    cleanup_performance_system
)
from src.serialization.protobuf_serializer import (
    MessageSchema,
    create_stock_data_schema,
    create_trade_signal_schema
)

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


async def demo_basic_usage():
    """æ¼”ç¤ºåŸºæœ¬ä½¿ç”¨æ–¹æ³•"""
    logger.info("=" * 60)
    logger.info("Demo 1: åŸºæœ¬ä½¿ç”¨æ–¹æ³•")
    logger.info("=" * 60)

    # åˆå§‹åŒ–æ€§èƒ½ç³»ç»Ÿ
    await initialize_performance_system()

    # 1. ä½¿ç”¨åç¨‹æ± æ‰§è¡Œä»»åŠ¡
    async def sample_task(x: int, y: int) -> int:
        await asyncio.sleep(0.1)
        return x + y

    result = await execute_with_pool(sample_task, 10, 20)
    logger.info(f"åç¨‹æ± æ‰§è¡Œç»“æœ: {result}")

    # 2. ä½¿ç”¨é€Ÿç‡é™åˆ¶æ‰§è¡Œä»»åŠ¡
    async def api_call(url: str) -> str:
        await asyncio.sleep(0.1)
        return f"Response from {url}"

    result = await rate_limited_execute(
        api_call,
        "http://api.example.com/data",
        resource_id="api_calls"
    )
    logger.info(f"é€Ÿç‡é™åˆ¶æ‰§è¡Œç»“æœ: {result}")

    # 3. åºåˆ—åŒ–æ•°æ®
    stock_data = {
        'symbol': '0700.HK',
        'timestamp': int(time.time()),
        'open': 100.5,
        'high': 105.0,
        'low': 99.0,
        'close': 103.5,
        'volume': 1000000
    }

    # æ³¨å†Œschema
    serializer = await get_serializer()
    stock_schema = create_stock_data_schema()
    serializer.register_schema(stock_schema)

    # åºåˆ—åŒ–
    serialized = await serialize_and_send(stock_data, "StockData", compress=True)
    logger.info(f"åºåˆ—åŒ–åå¤§å°: {len(serialized)} bytes")

    # ååºåˆ—åŒ–
    deserialized = await receive_and_deserialize(serialized, "StockData", decompress=True)
    logger.info(f"ååºåˆ—åŒ–ç»“æœ: {deserialized['symbol']}")

    logger.info("âœ“ åŸºæœ¬ä½¿ç”¨æ¼”ç¤ºå®Œæˆ\n")


async def demo_batch_processing():
    """æ¼”ç¤ºæ‰¹é‡å¤„ç†"""
    logger.info("=" * 60)
    logger.info("Demo 2: æ‰¹é‡å¤„ç†")
    logger.info("=" * 60)

    # æ¨¡æ‹Ÿå¤„ç†å¤§é‡è‚¡ç¥¨æ•°æ®
    async def process_stock_data(symbol_data: tuple):
        symbol, timestamp = symbol_data
        await asyncio.sleep(0.01)  # æ¨¡æ‹Ÿå¤„ç†æ—¶é—´
        return {
            'symbol': symbol,
            'timestamp': timestamp,
            'price': 100.0 + (timestamp % 10),
            'volume': 1000000 + (timestamp % 100000)
        }

    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    symbols = ['0700.HK', '0388.HK', '1398.HK', '0939.HK', '3988.HK']
    test_data = [(symbol, int(time.time()) + i) for i, symbol in enumerate(symbols * 20)]

    logger.info(f"å‡†å¤‡å¤„ç† {len(test_data)} æ¡æ•°æ®...")

    start = time.time()
    results = await batch_process(test_data, process_stock_data, batch_size=50)
    duration = time.time() - start

    logger.info(f"å®Œæˆå¤„ç†: {len(results)} æ¡æ•°æ®")
    logger.info(f"å¤„ç†æ—¶é—´: {duration:.2f} ç§’")
    logger.info(f"ååé‡: {len(results)/duration:.2f} ops/sec")
    logger.info(f"ç¤ºä¾‹ç»“æœ: {results[0]}")

    logger.info("âœ“ æ‰¹é‡å¤„ç†æ¼”ç¤ºå®Œæˆ\n")


async def demo_high_throughput():
    """æ¼”ç¤ºé«˜ååé‡å¤„ç†"""
    logger.info("=" * 60)
    logger.info("Demo 3: é«˜ååé‡å¤„ç†")
    logger.info("=" * 60)

    # åˆ›å»ºè¾“å…¥å’Œè¾“å‡ºé˜Ÿåˆ—
    input_queue = asyncio.Queue(maxsize=1000)
    output_queue = asyncio.Queue(maxsize=1000)

    # å¯åŠ¨é«˜ååé‡å¤„ç†å™¨
    processor_task = asyncio.create_task(
        high_throughput_processor(
            input_queue,
            lambda x: x * 2,
            output_queue,
            max_concurrent=100,
            resource_id="high_throughput"
        )
    )

    # ç”Ÿäº§æ•°æ®
    logger.info("ç”Ÿäº§æ•°æ®ä¸­...")
    for i in range(100):
        await input_queue.put(i)

    # ç­‰å¾…å¤„ç†å®Œæˆ
    await input_queue.join()

    # è·å–ç»“æœ
    results = []
    while not output_queue.empty():
        results.append(await output_queue.get())

    logger.info(f"å¤„ç†äº† {len(results)} æ¡æ•°æ®")
    logger.info(f"å‰10ä¸ªç»“æœ: {results[:10]}")

    # å–æ¶ˆå¤„ç†å™¨ä»»åŠ¡
    processor_task.cancel()
    try:
        await processor_task
    except asyncio.CancelledError:
        pass

    logger.info("âœ“ é«˜ååé‡å¤„ç†æ¼”ç¤ºå®Œæˆ\n")


async def demo_performance_monitoring():
    """æ¼”ç¤ºæ€§èƒ½ç›‘æ§"""
    logger.info("=" * 60)
    logger.info("Demo 4: æ€§èƒ½ç›‘æ§")
    logger.info("=" * 60)

    # æ‰§è¡Œä¸€äº›ä»»åŠ¡
    async def monitor_task(n):
        await asyncio.sleep(0.05)
        return n * 2

    # æ‰¹é‡æäº¤ä»»åŠ¡
    for batch in range(3):
        logger.info(f"æ‰§è¡Œç¬¬ {batch + 1} æ‰¹ä»»åŠ¡...")
        await batch_process(
            list(range(20)),
            monitor_task,
            batch_size=20
        )

        # è·å–æ€§èƒ½ç»Ÿè®¡
        stats = await get_performance_stats()

        # æ‰“å°åç¨‹æ± ç»Ÿè®¡
        pool_stats = stats.get('coroutine_pool', {})
        logger.info(
            f"åç¨‹æ±  - å·¥ä½œè€…: {pool_stats.get('total_workers', 0)}/"
            f"{pool_stats.get('active_workers', 0)}, "
            f"é˜Ÿåˆ—: {pool_stats.get('queue_size', 0)}/"
            f"{pool_stats.get('queue_max_size', 0)}"
        )

        # æ‰“å°èƒŒå‹ç»Ÿè®¡
        bp_stats = stats.get('backpressure', {})
        if 'resource_stats' in bp_stats:
            for resource, stat in bp_stats['resource_stats'].items():
                logger.info(
                    f"èƒŒå‹ - {resource}: "
                    f"{stat.get('requests_in_window', 0)} requests"
                )

        await asyncio.sleep(0.5)

    logger.info("âœ“ æ€§èƒ½ç›‘æ§æ¼”ç¤ºå®Œæˆ\n")


async def demo_benchmark():
    """æ¼”ç¤ºåŸºå‡†æµ‹è¯•"""
    logger.info("=" * 60)
    logger.info("Demo 5: åŸºå‡†æµ‹è¯•")
    logger.info("=" * 60)

    # å®šä¹‰åŸºå‡†æµ‹è¯•ä»»åŠ¡
    async def benchmark_task():
        # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
        result = sum(i * i for i in range(100))
        return result

    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    logger.info("è¿è¡Œååé‡åŸºå‡†æµ‹è¯•...")
    results = await benchmark_throughput(
        benchmark_task,
        iterations=1000,
        concurrency=100
    )

    logger.info("åŸºå‡†æµ‹è¯•ç»“æœ:")
    logger.info(f"  æ€»æ—¶é—´: {results['total_duration']:.2f} ç§’")
    logger.info(f"  ååé‡: {results['throughput']:.2f} ops/sec")
    logger.info(f"  å¹³å‡å»¶è¿Ÿ: {results['avg_latency']*1000:.2f} ms")
    logger.info(f"  å®Œæˆ: {results['completed']}")
    logger.info(f"  å¤±è´¥: {results['failed']}")

    logger.info("âœ“ åŸºå‡†æµ‹è¯•æ¼”ç¤ºå®Œæˆ\n")


async def demo_real_world_scenario():
    """æ¼”ç¤ºçœŸå®åœºæ™¯ï¼šè‚¡ç¥¨æ•°æ®å¤„ç†æµæ°´çº¿"""
    logger.info("=" * 60)
    logger.info("Demo 6: çœŸå®åœºæ™¯ - è‚¡ç¥¨æ•°æ®å¤„ç†æµæ°´çº¿")
    logger.info("=" * 60)

    # åˆ›å»ºschema
    stock_schema = create_stock_data_schema()
    trade_signal_schema = create_trade_signal_schema()

    serializer = await get_serializer()
    serializer.register_schema(stock_schema)
    serializer.register_schema(trade_signal_schema)

    async def fetch_stock_data(symbol: str) -> Dict:
        """è·å–è‚¡ç¥¨æ•°æ®"""
        await asyncio.sleep(0.05)  # æ¨¡æ‹ŸAPIè°ƒç”¨
        return {
            'symbol': symbol,
            'timestamp': int(time.time()),
            'open': 100.0,
            'high': 105.0,
            'low': 99.0,
            'close': 103.5,
            'volume': 1000000
        }

    async def analyze_signals(stock_data: Dict) -> Dict:
        """åˆ†æäº¤æ˜“ä¿¡å·"""
        await asyncio.sleep(0.02)
        # ç®€å•çš„ä¿¡å·ç”Ÿæˆ
        price_change = stock_data['close'] - stock_data['open']
        action = 'BUY' if price_change > 0 else 'SELL'
        return {
            'symbol': stock_data['symbol'],
            'action': action,
            'price': stock_data['close'],
            'quantity': 1000,
            'timestamp': stock_data['timestamp'],
            'strategy': 'PRICE_CHANGE',
            'confidence': 0.8
        }

    async def send_signal(signal: Dict) -> bool:
        """å‘é€äº¤æ˜“ä¿¡å·"""
        # åºåˆ—åŒ–ä¿¡å·
        serialized = await serialize_and_send(signal, "TradeSignal", compress=True)
        logger.debug(f"å‘é€ä¿¡å·: {signal['symbol']} {signal['action']} (å¤§å°: {len(serialized)} bytes)")
        return True

    # æ¨¡æ‹Ÿè‚¡ç¥¨åˆ—è¡¨
    symbols = ['0700.HK', '0388.HK', '1398.HK', '0939.HK', '3988.HK']

    logger.info("å¯åŠ¨å¤„ç†æµæ°´çº¿...")

    # é˜¶æ®µ1: è·å–æ•°æ®
    logger.info("é˜¶æ®µ1: è·å–è‚¡ç¥¨æ•°æ®...")
    start = time.time()
    stock_data_list = await batch_process(symbols, fetch_stock_data, batch_size=len(symbols))
    logger.info(f"è·å– {len(stock_data_list)} åªè‚¡ç¥¨æ•°æ®")

    # é˜¶æ®µ2: åˆ†æä¿¡å·
    logger.info("é˜¶æ®µ2: åˆ†æäº¤æ˜“ä¿¡å·...")
    signals = await batch_process(stock_data_list, analyze_signals, batch_size=len(stock_data_list))
    logger.info(f"ç”Ÿæˆ {len(signals)} ä¸ªäº¤æ˜“ä¿¡å·")

    # é˜¶æ®µ3: å‘é€ä¿¡å·ï¼ˆé€Ÿç‡é™åˆ¶ï¼‰
    logger.info("é˜¶æ®µ3: å‘é€äº¤æ˜“ä¿¡å·...")
    for signal in signals:
        await rate_limited_execute(
            send_signal,
            signal,
            resource_id="signal_sender",
            priority=5
        )
    logger.info("æ‰€æœ‰ä¿¡å·å·²å‘é€")

    # ç»Ÿè®¡
    duration = time.time() - start
    logger.info(f"\næµæ°´çº¿å®Œæˆç»Ÿè®¡:")
    logger.info(f"  æ€»æ—¶é—´: {duration:.2f} ç§’")
    logger.info(f"  ååé‡: {len(signals)/duration:.2f} signals/sec")
    logger.info(f"  æ•°æ®é‡: {len(stock_data_list)} æ¡")
    logger.info(f"  ä¿¡å·é‡: {len(signals)} ä¸ª")

    # æ˜¾ç¤ºç¤ºä¾‹ä¿¡å·
    logger.info(f"\nç¤ºä¾‹äº¤æ˜“ä¿¡å·:")
    logger.info(f"  {json.dumps(signals[0], indent=2, default=str)}")

    logger.info("âœ“ çœŸå®åœºæ™¯æ¼”ç¤ºå®Œæˆ\n")


async def main():
    """ä¸»å‡½æ•°"""
    logger.info("ğŸš€ Sprint 4 æ€§èƒ½ä¼˜åŒ– - å®Œæ•´æ¼”ç¤º")
    logger.info("æ¼”ç¤ºåç¨‹æ±  + Protocol Buffers + èƒŒå‹æ§åˆ¶çš„é›†æˆä½¿ç”¨\n")

    try:
        # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½
        await demo_basic_usage()
        await asyncio.sleep(1)

        await demo_batch_processing()
        await asyncio.sleep(1)

        await demo_high_throughput()
        await asyncio.sleep(1)

        await demo_performance_monitoring()
        await asyncio.sleep(1)

        await demo_benchmark()
        await asyncio.sleep(1)

        await demo_real_world_scenario()

        # æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š
        logger.info("=" * 60)
        logger.info("æœ€ç»ˆæ€§èƒ½æŠ¥å‘Š")
        logger.info("=" * 60)

        stats = await get_performance_stats()
        logger.info(json.dumps(stats, indent=2, default=str))

        logger.info("\nâœ… æ‰€æœ‰æ¼”ç¤ºå®Œæˆ!")
        logger.info("âœ“ åç¨‹æ± ç®¡ç† - åŠ¨æ€æ‰©ç¼©å®¹")
        logger.info("âœ“ èƒŒå‹æ§åˆ¶ - é€Ÿç‡é™åˆ¶å’Œç†”æ–­å™¨")
        logger.info("âœ“ Protocol Buffers - é«˜æ•ˆåºåˆ—åŒ–")
        logger.info("âœ“ æ€§èƒ½ä¼˜åŒ– - 200K msg/s ç›®æ ‡")

    except Exception as e:
        logger.error(f"æ¼”ç¤ºè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}", exc_info=True)

    finally:
        # æ¸…ç†èµ„æº
        logger.info("\næ¸…ç†æ€§èƒ½ç³»ç»Ÿ...")
        await cleanup_performance_system()
        logger.info("æ¸…ç†å®Œæˆ!")


if __name__ == "__main__":
    asyncio.run(main())
