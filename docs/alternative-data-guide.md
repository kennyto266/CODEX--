# æ›¿ä»£æ•¸æ“šæ¡†æ¶ - ç”¨æˆ¶æŒ‡å—

## ğŸ“š ç›®éŒ„

1. [ç°¡ä»‹](#ç°¡ä»‹)
2. [å¿«é€Ÿé–‹å§‹](#å¿«é€Ÿé–‹å§‹)
3. [æ ¸å¿ƒçµ„ä»¶](#æ ¸å¿ƒçµ„ä»¶)
4. [æ•¸æ“šç®¡é“](#æ•¸æ“šç®¡é“)
5. [å›æ¸¬é›†æˆ](#å›æ¸¬é›†æˆ)
6. [ä¿¡è™Ÿæ­¸å› ](#ä¿¡è™Ÿæ­¸å› )
7. [API åƒè€ƒ](#api-åƒè€ƒ)
8. [å¸¸è¦‹å•é¡Œ](#å¸¸è¦‹å•é¡Œ)
9. [æ€§èƒ½èª¿å„ª](#æ€§èƒ½èª¿å„ª)

---

## ç°¡ä»‹

æ›¿ä»£æ•¸æ“šæ¡†æ¶æ˜¯ä¸€å€‹å®Œæ•´çš„ç³»çµ±ï¼Œç”¨æ–¼å°‡æ›¿ä»£æ•¸æ“šï¼ˆå¦‚ HIBOR åˆ©ç‡ã€è¨ªå®¢åˆ°é”é‡ã€é›¶å”®éŠ·å”®ç­‰ï¼‰èˆ‡å‚³çµ±çš„åƒ¹æ ¼æ•¸æ“šç›¸çµåˆï¼Œä»¥æ”¹é€²é‡åŒ–äº¤æ˜“ç­–ç•¥ã€‚

### ä¸»è¦ç‰¹æ€§

- âœ… **å®Œæ•´çš„æ•¸æ“šç®¡é“**: æ¸…ç†ã€å°é½Šã€æ¨™æº–åŒ–ã€è©•åˆ†
- âœ… **å›æ¸¬é›†æˆ**: å°‡æ›¿ä»£æ•¸æ“šç´å…¥å›æ¸¬å¼•æ“
- âœ… **ä¿¡è™Ÿæ­¸å› **: è·Ÿè¹¤æ¯å€‹ä¿¡è™Ÿæºçš„è²¢ç»
- âœ… **æ€§èƒ½æŒ‡æ¨™**: è¶…é 600K è¡Œ/ç§’çš„è™•ç†é€Ÿåº¦
- âœ… **API æœå‹™**: RESTful ç«¯é»ç”¨æ–¼çµæœç®¡ç†
- âœ… **é«˜æ•ˆè¨˜æ†¶é«”**: 0.01% è¨˜æ†¶é«”å¢é•·ç‡

### æ”¯æŒçš„æ›¿ä»£æ•¸æ“šæº

| é¡åˆ¥ | æŒ‡æ¨™ | é »ç‡ |
|------|------|------|
| **è²¨å¹£æ”¿ç­–** | HIBOR åˆ©ç‡ (O/N, 1M, 3M, 6M, 12M) | æ¯æ—¥ |
| **æˆ¿ç”¢å¸‚å ´** | æˆ¿åƒ¹ã€ç§Ÿè³ƒã€äº¤æ˜“é‡ã€å›å ±ç‡ | æ¯æœˆ |
| **é›¶å”®æ•¸æ“š** | é›¶å”®ç¸½é¡ã€è¡£è‘—ã€è¶…å¸‚ã€é¤é£²ç­‰ | æ¯æœˆ |
| **ç¶“æ¿ŸæŒ‡æ¨™** | GDPã€å°±æ¥­ã€æ¶ˆè²»è€…ä¿¡å¿ƒ | æ¯å­£åº¦ |
| **è¨ªå®¢æ•¸æ“š** | ç¸½åˆ°é”é‡ã€å…§åœ°ã€åœ‹éš› | æ¯æ—¥ |
| **äº¤æ˜“æ•¸æ“š** | é€²å£ã€å‡ºå£ã€è²¿æ˜“å¹³è¡¡ | æ¯æœˆ |
| **äº¤é€šæµé‡** | é“è·¯æµé‡ã€å¹³å‡é€Ÿåº¦ã€æ“å µæŒ‡æ•¸ | å¯¦æ™‚ |
| **å…¬å…±äº¤é€š** | MTR ä¹˜å®¢ã€é«˜å³°æ™‚æ®µ | æ¯æ—¥ |
| **é‚Šå¢ƒé€šé—œ** | å±…æ°‘é€²å‡ºã€è¨ªå®¢é€²å‡º | æ¯æ—¥ |

---

## å¿«é€Ÿé–‹å§‹

### å®‰è£

```bash
# 1. å…‹éš†æˆ–é€²å…¥é …ç›®ç›®éŒ„
cd CODEX--

# 2. å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv .venv
.venv\Scripts\activate  # Windows

# 3. å®‰è£ä¾è³´
pip install -r requirements.txt
```

### åŸºæœ¬ä½¿ç”¨

```python
import pandas as pd
from src.data_pipeline import DataCleaner, TemporalAligner, DataNormalizer, QualityScorer

# 1. æº–å‚™æ•¸æ“š
dates = pd.date_range('2023-01-01', periods=365)
price_data = pd.DataFrame({
    'open': ...,
    'high': ...,
    'low': ...,
    'close': ...,
    'volume': ...,
}, index=dates)

alt_data = {
    'hibor_rate': pd.Series(..., index=dates),
    'visitor_arrivals': pd.Series(..., index=dates),
}

# 2. æ¸…ç†æ•¸æ“š
cleaner = DataCleaner()
cleaned = cleaner.clean(price_data)

# 3. å°é½æ™‚é–“
aligner = TemporalAligner()
aligned = aligner.align_to_trading_days(cleaned)

# 4. æ¨™æº–åŒ–
normalizer = DataNormalizer()
normalized = normalizer.fit_transform(aligned)

# 5. è©•åˆ†è³ªé‡
scorer = QualityScorer()
quality = scorer.calculate_overall_grade(normalized)

# 6. å®Œæ•´ç®¡é“è™•ç†
from src.data_pipeline import PipelineProcessor
pipeline = PipelineProcessor()
result = pipeline.process(normalized)
```

---

## æ ¸å¿ƒçµ„ä»¶

### 1. DataCleaner (æ•¸æ“šæ¸…ç†)

è² è²¬è™•ç†ç¼ºå¤±å€¼ã€ç•°å¸¸å€¼å’Œæ•¸æ“šé©—è­‰ã€‚

```python
from src.data_pipeline import DataCleaner

cleaner = DataCleaner(
    missing_threshold=0.1,      # å…è¨± 10% ç¼ºå¤±
    outlier_method='iqr',        # ä½¿ç”¨ IQR æ–¹æ³•
    fill_method='forward_fill'   # å‰å‘å¡«å……
)

cleaned = cleaner.clean(df)

# ç²å–æ¸…ç†å ±å‘Š
report = cleaner.get_report()
```

**æ€§èƒ½**: 631K è¡Œ/ç§’

**ä¸»è¦æ–¹æ³•**:
- `clean(df)`: åŸ·è¡Œå®Œæ•´æ¸…ç†
- `validate_data_quality(df)`: é©—è­‰è³ªé‡
- `get_report()`: ç²å–è©³ç´°å ±å‘Š

### 2. TemporalAligner (æ™‚é–“å°é½)

ç¢ºä¿æ‰€æœ‰æ™‚é–“åºåˆ—å°é½¡åˆ°äº¤æ˜“æ—¥æœŸã€‚

```python
from src.data_pipeline import TemporalAligner

aligner = TemporalAligner()

# å°é½åˆ°äº¤æ˜“æ—¥æœŸ
aligned = aligner.align_to_trading_days(df, date_column=None)

# èª¿æ•´é »ç‡
resampled = aligner.resample_data(df, freq='W')  # é€±é »ç‡

# ç”Ÿæˆå»¶é²ç‰¹å¾µ
lagged = aligner.generate_lagged_features(df, lags=[1, 5, 20])
```

**æ€§èƒ½**: 634K è¡Œ/ç§’

**æ¸¯è‚¡äº¤æ˜“æ—¥æœŸç‰¹æ€§**:
- è‡ªå‹•æ’é™¤å‘¨æœ«
- è‡ªå‹•æ’é™¤å…¬çœ¾å‡æœŸ
- æ”¯æŒç‰¹æ®Šå‡æœŸé…ç½®

### 3. DataNormalizer (æ•¸æ“šæ¨™æº–åŒ–)

å°‡æ•¸æ“šè®Šæ›ç‚ºå¯æ¯”è¼ƒçš„å°ºåº¦ã€‚

```python
from src.data_pipeline import DataNormalizer

normalizer = DataNormalizer()

# Fit å’Œ Transform
normalized = normalizer.fit_transform(df)

# åå‘è®Šæ›
original = normalizer.inverse_transform(normalized)

# ç‰¹å®šåˆ—æ¨™æº–åŒ–
zscore_norm = normalizer.zscore_normalize(df, columns=['close'])
minmax_norm = normalizer.minmax_scale(df, columns=['volume'])
```

**æ€§èƒ½**: 4M è¡Œ/ç§’

**æ”¯æŒçš„æ–¹æ³•**:
- Z-score: `(x - mean) / std`
- Min-Max: `(x - min) / (max - min)`
- Log: `log(x)`

### 4. QualityScorer (è³ªé‡è©•åˆ†)

è©•ä¼°æ•¸æ“šçš„å®Œæ•´æ€§ã€æ–°é®®åº¦å’Œä¸€è‡´æ€§ã€‚

```python
from src.data_pipeline import QualityScorer

scorer = QualityScorer()

# è¨ˆç®—æ•´é«”ç­‰ç´š
grade = scorer.calculate_overall_grade(df)
# Returns: {'grade': 'A', 'score': 0.95, 'completeness': 0.98, ...}

# è¨ˆç®—å€‹åˆ¥æŒ‡æ¨™
completeness = scorer.calculate_completeness_score(df['close'])
freshness = scorer.calculate_freshness_score(df)
```

**è³ªé‡ç­‰ç´š**:
- **A+**: 95-100% (å„ªç§€)
- **A**: 90-95% (å¾ˆå¥½)
- **B**: 80-90% (å¥½)
- **C**: 70-80% (å¯æ¥å—)
- **D**: < 70% (ä¸å¯æ¥å—)

### 5. PipelineProcessor (ç®¡é“è™•ç†)

ä¸²è¯æ‰€æœ‰æ­¥é©Ÿçš„ä¸€ç«™å¼è§£æ±ºæ–¹æ¡ˆã€‚

```python
from src.data_pipeline import PipelineProcessor

pipeline = PipelineProcessor()

# è™•ç†å–®å€‹æ•¸æ“šé›†
result = pipeline.process(normalized_data)

# ä½¿ç”¨é…ç½®è™•ç†
config = {
    'normalize': True,
    'calculate_quality': True,
    'generate_features': True
}
result = pipeline.process_with_config(data, config)
```

**æ€§èƒ½**: 20ms for 5000 è¡Œ

---

## æ•¸æ“šç®¡é“

### å®Œæ•´å·¥ä½œæµ

```python
# æ­¥é©Ÿ 1: æ¸…ç†
cleaner = DataCleaner()
cleaned = cleaner.clean(raw_data)

# æ­¥é©Ÿ 2: å°é½
aligner = TemporalAligner()
aligned = aligner.align_to_trading_days(cleaned)

# æ­¥é©Ÿ 3: æ¨™æº–åŒ–
normalizer = DataNormalizer()
normalized = normalizer.fit_transform(aligned)

# æ­¥é©Ÿ 4: è©•åˆ†
scorer = QualityScorer()
quality = scorer.calculate_overall_grade(normalized)
print(f"Quality Grade: {quality['grade']}")

# æ­¥é©Ÿ 5: è™•ç†
pipeline = PipelineProcessor()
processed = pipeline.process(normalized)
```

### èˆ‡æ›¿ä»£æ•¸æ“šæ•´åˆ

```python
# å°é½æ›¿ä»£æ•¸æ“š
alt_data = {
    'hibor': pd.Series(..., index=dates),
    'visitors': pd.Series(..., index=dates),
}

# å°é½æ›¿ä»£æ•¸æ“šåˆ°ç›¸åŒæ—¥æœŸ
aligner = TemporalAligner()
aligned_alt = {}
for name, series in alt_data.items():
    aligned_alt[name] = aligner.align_to_trading_days(
        pd.DataFrame({name: series})
    )

# åˆä½µæ•¸æ“šé›†
merged = pd.concat([normalized, aligned_alt['hibor']], axis=1)
```

---

## å›æ¸¬é›†æˆ

### ä½¿ç”¨æ›¿ä»£æ•¸æ“šé€²è¡Œå›æ¸¬

```python
from src.backtest import AltDataBacktestEngine
from src.backtest.base_backtest import BacktestConfig
from datetime import date

# é…ç½®
config = BacktestConfig(
    strategy_name='AltDataSignal',
    symbols=['0700.HK'],
    start_date=date(2023, 1, 1),
    end_date=date(2023, 12, 31),
    initial_capital=100000.0,
    benchmark='HSI.HK'
)

# å‰µå»ºå¼•æ“
engine = AltDataBacktestEngine(config)
engine.add_backtest_data('0700.HK', price_data)

# å®šç¾©ç­–ç•¥
def strategy(data_slice, signals):
    if signals['close'][-1] > data_slice['close'].mean():
        return {'action': 'buy', 'quantity': 100}
    return {'action': 'sell', 'quantity': 100}

# åŸ·è¡Œå›æ¸¬
result = await engine.run_backtest_with_alt_data(
    strategy_func=strategy,
    alt_data_signals={
        'hibor': hibor_series,
        'visitors': visitor_series,
    },
    signal_merge_strategy='weighted'  # 'weighted', 'voting', 'max'
)

print(f"Total Return: {result.total_return:.2%}")
print(f"Sharpe Ratio: {result.sharpe_ratio:.2f}")
```

### ä¿¡è™Ÿåˆä½µç­–ç•¥

| ç­–ç•¥ | èªªæ˜ | ç”¨é€” |
|------|------|------|
| **weighted** | åŠ æ¬Šå¹³å‡ | ä¸€èˆ¬ç”¨é€” |
| **voting** | å¤šæ•¸æŠ•ç¥¨ | ä¿¡è™Ÿä¸€è‡´æ€§ |
| **max** | æœ€å¤§çµ•å°å€¼ | å¼·ä¿¡è™Ÿæª¢æ¸¬ |

---

## ä¿¡è™Ÿæ­¸å› 

### è¿½è¹¤ä¿¡è™Ÿè²¢ç»

```python
from src.backtest import SignalAttributionAnalyzer

analyzer = SignalAttributionAnalyzer()

# è¨ˆç®—ä¿¡è™Ÿæº–ç¢ºåº¦
accuracy = analyzer.calculate_signal_accuracy(trades)
# Returns: {'overall_accuracy': 0.68, 'price_only': 0.65, ...}

# ç”Ÿæˆä¿¡è™Ÿåˆ†æ
breakdown = analyzer.generate_signal_breakdown(trades)
print(f"åƒ¹æ ¼ä¿¡è™Ÿ: {breakdown.price_metrics.win_rate:.2%}")
print(f"æ›¿ä»£æ•¸æ“š: {breakdown.alt_data_metrics.win_rate:.2%}")
print(f"çµ„åˆä¿¡è™Ÿ: {breakdown.combined_metrics.win_rate:.2%}")

# è¨ˆç®—ä¿¡è™Ÿæ•ˆç‡
efficiency = analyzer.calculate_signal_efficiency(trades)
```

### ä¿¡è™Ÿé©—è­‰

```python
from src.backtest import SignalValidator

validator = SignalValidator()

# æ¨£æœ¬å¤–æ¸¬è©¦
train_trades = trades[:int(len(trades)*0.7)]
test_trades = trades[int(len(trades)*0.7):]

# æª¢æ¸¬éåº¦æ“¬åˆ
overfitting = validator.detect_overfitting(
    train_metrics={'sharpe': 1.5, 'win_rate': 0.65},
    test_metrics={'sharpe': 0.8, 'win_rate': 0.55}
)

if overfitting.is_overfitted:
    print(f"Overfitting Level: {overfitting.level}")
    print(f"Risk Score: {overfitting.risk_score:.2f}")
```

---

## API åƒè€ƒ

### REST API ç«¯é»

#### ç²å–å›æ¸¬çµæœ

```bash
GET /api/dashboard/backtest/{result_id}

Response:
{
  "metadata": {
    "result_id": "test_001",
    "symbol": "0700.HK",
    "strategy_name": "AltDataSignal"
  },
  "metrics": {
    "total_return": 0.15,
    "sharpe_ratio": 1.67,
    "max_drawdown": -0.08
  }
}
```

#### æ›¿ä»£æ•¸æ“šåˆ†æ

```bash
GET /api/dashboard/backtest/{result_id}/alt-data-analysis

Response:
{
  "signal_timeline": [
    {
      "timestamp": "2023-01-01",
      "signal_type": "buy",
      "source": "combined",
      "pnl": 500.0
    }
  ],
  "signal_statistics": {
    "total_signals": 50,
    "buy_signals": 25,
    "win_rate": 0.7
  },
  "source_breakdown": {
    "price_only": 15,
    "alt_data_only": 10,
    "combined": 25
  }
}
```

#### æ¯”è¼ƒçµæœ

```bash
POST /api/dashboard/backtest/{result_id_with_alt}/compare/{result_id_without_alt}

Response:
{
  "result_with_alt_data": {
    "sharpe_ratio": 2.0,
    "total_return": 0.20
  },
  "result_without_alt_data": {
    "sharpe_ratio": 1.2,
    "total_return": 0.12
  },
  "improvement": {
    "sharpe_ratio_improvement_pct": 66.7,
    "return_improvement_pct": 66.7
  }
}
```

#### åˆ—è¡¨çµæœ

```bash
GET /api/dashboard/backtest/list?symbol=0700.HK&limit=10

Response:
[
  {
    "result_id": "test_001",
    "symbol": "0700.HK",
    "strategy_name": "AltDataSignal",
    "created_at": "2025-01-01T12:00:00"
  }
]
```

---

## å¸¸è¦‹å•é¡Œ

### Q1: å¦‚ä½•è™•ç†ç¼ºå¤±çš„æ›¿ä»£æ•¸æ“šï¼Ÿ

**A**: ä½¿ç”¨æ•¸æ“šæ¸…ç†å™¨çš„å‰å‘å¡«å……æˆ–ç·šæ€§æ’å€¼:

```python
cleaner = DataCleaner(fill_method='forward_fill')
filled = cleaner.clean(df_with_missing)
```

### Q2: å“ªäº›ä¿¡è™Ÿåˆä½µç­–ç•¥æœ€æœ‰æ•ˆï¼Ÿ

**A**: é€™å–æ±ºæ–¼æ‚¨çš„æ•¸æ“š:
- **Weighted**: é©åˆé€£çºŒä¿¡è™Ÿ
- **Voting**: é©åˆé›¢æ•£ä¿¡è™Ÿ
- **Max**: é©åˆæª¢æ¸¬å¼·ä¿¡è™Ÿ

### Q3: å¦‚ä½•è©•ä¼°æ›¿ä»£æ•¸æ“šçš„æœ‰æ•ˆæ€§ï¼Ÿ

**A**: ä½¿ç”¨ä¿¡è™Ÿæ­¸å› åˆ†æ:

```python
analyzer = SignalAttributionAnalyzer()
breakdown = analyzer.generate_signal_breakdown(trades)

# æ¯”è¼ƒä¸åŒä¿¡è™Ÿæºçš„å‹ç‡
alt_win_rate = breakdown.alt_data_metrics.win_rate
price_win_rate = breakdown.price_metrics.win_rate

if alt_win_rate > price_win_rate:
    print("æ›¿ä»£æ•¸æ“šæ›´æœ‰æ•ˆ!")
```

### Q4: ç³»çµ±èƒ½è™•ç†å¤šå°‘æ•¸æ“šï¼Ÿ

**A**: ç³»çµ±ç¶“éå„ªåŒ–ï¼Œå¯ä»¥è™•ç†:
- **æ­·å²æ•¸æ“š**: 10+ å¹´çš„æ—¥é »ç‡æ•¸æ“š
- **ååé‡**: 600K+ è¡Œ/ç§’
- **è¨˜æ†¶é«”**: 10K è¡Œæ•¸æ“š < 10MB

### Q5: å¦‚ä½•é›†æˆè‡ªå®šç¾©æ›¿ä»£æ•¸æ“šæºï¼Ÿ

**A**: å‰µå»ºè‡ªå®šç¾©é©é…å™¨:

```python
from src.data_adapters.base_adapter import BaseAdapter

class CustomDataAdapter(BaseAdapter):
    def fetch_data(self, symbol, start_date, end_date):
        # å¯¦ç¾æ‚¨çš„æ•¸æ“šç²å–é‚è¼¯
        return data_dataframe
```

---

## æ€§èƒ½èª¿å„ª

### å„ªåŒ–æç¤º

1. **ä½¿ç”¨å‘é‡åŒ–æ“ä½œ**
   ```python
   # âœ… å¥½ - å‘é‡åŒ–
   normalized = (df - df.mean()) / df.std()

   # âŒ ä¸å¥½ - å¾ªç’°
   for i in range(len(df)):
       normalized[i] = (df[i] - df.mean()) / df.std()
   ```

2. **é å…ˆå°é½æ‰€æœ‰æ•¸æ“š**
   ```python
   # ç¢ºä¿æ‰€æœ‰æ•¸æ“šåœ¨ç›¸åŒæ—¥æœŸç¯„åœå…§
   aligned = aligner.align_to_trading_days(df)
   ```

3. **ä½¿ç”¨é©ç•¶çš„æ•¸æ“šé¡å‹**
   ```python
   df = df.astype({
       'close': 'float32',    # ä¸éœ€è¦é›™ç²¾åº¦
       'volume': 'int32',
       'symbol': 'category'   # ç¯€çœå…§å­˜
   })
   ```

4. **æ‰¹é‡è™•ç†å¤§å‹æ•¸æ“šé›†**
   ```python
   batch_size = 1000
   for i in range(0, len(df), batch_size):
       batch = df[i:i+batch_size]
       process_batch(batch)
   ```

### æ€§èƒ½åŸºæº–

| æ“ä½œ | ååé‡ | èªªæ˜ |
|------|--------|------|
| æ•¸æ“šæ¸…ç† | 631K è¡Œ/s | å–®æ ¸å¿ƒ |
| æ™‚é–“å°é½ | 634K è¡Œ/s | äº¤æ˜“æ—¥éæ¿¾ |
| æ¨™æº–åŒ– | 4M è¡Œ/s | å‘é‡åŒ– |
| è³ªé‡è©•åˆ† | < 1ms | å®Œæ•´æ•¸æ“šé›† |
| API å­˜å„² | 282 çµæœ/s | SQLite + JSON |
| API æª¢ç´¢ | 1M çµæœ/s | å…§å­˜ç·©å­˜ |

---

## éœ€è¦å¹«åŠ©ï¼Ÿ

- ğŸ“§ æŸ¥çœ‹éŒ¯èª¤æ—¥èªŒ: `quant_system.log`
- ğŸ’¬ æª¢æŸ¥ README.md äº†è§£åŸºæœ¬è¨­ç½®
- ğŸ” æŸ¥çœ‹å–®å…ƒæ¸¬è©¦ç¤ºä¾‹: `tests/test_*.py`
- ğŸ“š æŸ¥çœ‹ API æ–‡æª”: `http://localhost:8001/docs`

---

**æœ€å¾Œæ›´æ–°**: 2025-10-25
**ç‰ˆæœ¬**: 1.0
**ç¶­è­·è€…**: Claude Code AI System
