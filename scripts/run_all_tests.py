#!/usr/bin/env python3
"""
æ¸¯è‚¡é‡åŒ–äº¤æ˜“AI Agentç³»ç»Ÿ - è¿è¡Œæ‰€æœ‰æµ‹è¯•è„šæœ¬

æœ¬è„šæœ¬ç»Ÿä¸€è¿è¡Œæ‰€æœ‰æµ‹è¯•å’ŒéªŒè¯è„šæœ¬ï¼ŒåŒ…æ‹¬ï¼š
- æœ€ç»ˆé›†æˆæµ‹è¯•
- ç³»ç»ŸéªŒè¯
- ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•
"""

import asyncio
import json
import logging
import os
import sys
import time
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

# å¯¼å…¥æµ‹è¯•è„šæœ¬
from final_integration_test import FinalIntegrationTest
from system_validation import SystemValidator
from production_simulation import ProductionSimulator


class TestRunner:
    """æµ‹è¯•è¿è¡Œå™¨"""
    
    def __init__(self):
        self.logger = self._setup_logger()
        self.test_results = {}
        self.start_time = datetime.now()
        
        # æµ‹è¯•è„šæœ¬é…ç½®
        self.test_scripts = [
            {
                "name": "final_integration_test",
                "description": "æœ€ç»ˆé›†æˆæµ‹è¯•å’Œç³»ç»ŸéªŒè¯",
                "class": FinalIntegrationTest,
                "enabled": True
            },
            {
                "name": "system_validation",
                "description": "ç³»ç»ŸéªŒè¯",
                "class": SystemValidator,
                "enabled": True
            },
            {
                "name": "production_simulation",
                "description": "ç”Ÿäº§ç¯å¢ƒæ¨¡æ‹Ÿæµ‹è¯•",
                "class": ProductionSimulator,
                "enabled": True
            }
        ]
        
        self.logger.info("æµ‹è¯•è¿è¡Œå™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _setup_logger(self) -> logging.Logger:
        """è®¾ç½®æ—¥å¿—è®°å½•å™¨"""
        logger = logging.getLogger("test_runner")
        logger.setLevel(logging.INFO)
        
        # åˆ›å»ºæ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setLevel(logging.INFO)
        
        # åˆ›å»ºæ–‡ä»¶å¤„ç†å™¨
        log_file = project_root / "logs" / "test_runner.log"
        log_file.parent.mkdir(parents=True, exist_ok=True)
        file_handler = logging.FileHandler(log_file)
        file_handler.setLevel(logging.DEBUG)
        
        # åˆ›å»ºæ ¼å¼å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        console_handler.setFormatter(formatter)
        file_handler.setFormatter(formatter)
        
        # æ·»åŠ å¤„ç†å™¨
        logger.addHandler(console_handler)
        logger.addHandler(file_handler)
        
        return logger
    
    async def run_all_tests(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
        self.logger.info("å¼€å§‹è¿è¡Œæ‰€æœ‰æµ‹è¯•")
        
        try:
            # è¿è¡Œæ¯ä¸ªæµ‹è¯•è„šæœ¬
            for test_script in self.test_scripts:
                if not test_script["enabled"]:
                    self.logger.info(f"è·³è¿‡æµ‹è¯•: {test_script['name']}")
                    continue
                
                await self._run_single_test(test_script)
            
            # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            comprehensive_report = self._generate_comprehensive_report()
            
            self.logger.info("æ‰€æœ‰æµ‹è¯•è¿è¡Œå®Œæˆ")
            return comprehensive_report
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•è¿è¡Œå¤±è´¥: {e}", exc_info=True)
            raise
    
    async def _run_single_test(self, test_script: Dict[str, Any]):
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        test_name = test_script["name"]
        test_description = test_script["description"]
        test_class = test_script["class"]
        
        self.logger.info(f"å¼€å§‹è¿è¡Œæµ‹è¯•: {test_description}")
        
        try:
            # åˆ›å»ºæµ‹è¯•å®ä¾‹
            test_instance = test_class()
            
            # è¿è¡Œæµ‹è¯•
            start_time = time.time()
            
            if test_name == "final_integration_test":
                result = await test_instance.run_complete_test_suite()
            elif test_name == "system_validation":
                result = await test_instance.run_validation()
            elif test_name == "production_simulation":
                result = await test_instance.run_simulation()
            else:
                raise ValueError(f"æœªçŸ¥çš„æµ‹è¯•ç±»å‹: {test_name}")
            
            end_time = time.time()
            duration = end_time - start_time
            
            # ä¿å­˜æµ‹è¯•ç»“æœ
            self.test_results[test_name] = {
                "status": "COMPLETED",
                "description": test_description,
                "duration": duration,
                "result": result,
                "timestamp": datetime.now().isoformat()
            }
            
            self.logger.info(f"æµ‹è¯•å®Œæˆ: {test_description} (è€—æ—¶: {duration:.2f}ç§’)")
            
        except Exception as e:
            self.logger.error(f"æµ‹è¯•å¤±è´¥: {test_description} - {e}", exc_info=True)
            
            self.test_results[test_name] = {
                "status": "FAILED",
                "description": test_description,
                "duration": 0,
                "error": str(e),
                "timestamp": datetime.now().isoformat()
            }
    
    def _generate_comprehensive_report(self) -> Dict[str, Any]:
        """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
        end_time = datetime.now()
        total_duration = (end_time - self.start_time).total_seconds()
        
        # ç»Ÿè®¡æµ‹è¯•ç»“æœ
        total_tests = len(self.test_results)
        completed_tests = sum(1 for result in self.test_results.values() if result["status"] == "COMPLETED")
        failed_tests = sum(1 for result in self.test_results.values() if result["status"] == "FAILED")
        
        # è®¡ç®—æ€»ä½“æˆåŠŸç‡
        overall_success_rate = 0
        if completed_tests > 0:
            # ä»å„ä¸ªæµ‹è¯•çš„ç»“æœä¸­æå–æˆåŠŸç‡
            success_rates = []
            for test_name, test_result in self.test_results.items():
                if test_result["status"] == "COMPLETED" and "result" in test_result:
                    result = test_result["result"]
                    if "test_summary" in result:
                        success_rates.append(result["test_summary"]["success_rate"])
                    elif "validation_summary" in result:
                        success_rates.append(result["validation_summary"]["success_rate"])
                    elif "simulation_summary" in result:
                        success_rates.append(result["simulation_summary"]["success_rate"])
            
            if success_rates:
                overall_success_rate = sum(success_rates) / len(success_rates)
        
        # ç”Ÿæˆç»¼åˆæŠ¥å‘Š
        comprehensive_report = {
            "comprehensive_summary": {
                "total_tests": total_tests,
                "completed_tests": completed_tests,
                "failed_tests": failed_tests,
                "overall_success_rate": overall_success_rate,
                "total_duration": total_duration,
                "start_time": self.start_time.isoformat(),
                "end_time": end_time.isoformat()
            },
            "test_results": self.test_results,
            "recommendations": self._generate_comprehensive_recommendations(),
            "metadata": {
                "test_runner_version": "1.0.0",
                "system_version": "1.0.0",
                "python_version": sys.version,
                "platform": os.name
            }
        }
        
        # ä¿å­˜ç»¼åˆæŠ¥å‘Šåˆ°æ–‡ä»¶
        report_file = project_root / "logs" / "comprehensive_test_report.json"
        with open(report_file, 'w', encoding='utf-8') as f:
            json.dump(comprehensive_report, f, indent=2, ensure_ascii=False)
        
        self.logger.info(f"ç»¼åˆæŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")
        
        return comprehensive_report
    
    def _generate_comprehensive_recommendations(self) -> List[str]:
        """ç”Ÿæˆç»¼åˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        # åŸºäºæµ‹è¯•ç»“æœç”Ÿæˆå»ºè®®
        for test_name, test_result in self.test_results.items():
            if test_result["status"] == "FAILED":
                recommendations.append(f"ä¿®å¤ {test_name} æµ‹è¯•å¤±è´¥çš„é—®é¢˜")
            elif test_result["status"] == "COMPLETED" and "result" in test_result:
                result = test_result["result"]
                if "recommendations" in result:
                    recommendations.extend(result["recommendations"])
        
        # é€šç”¨å»ºè®®
        recommendations.extend([
            "å®šæœŸè¿è¡Œå®Œæ•´çš„æµ‹è¯•å¥—ä»¶ä»¥ç¡®ä¿ç³»ç»Ÿè´¨é‡",
            "å»ºç«‹æŒç»­é›†æˆå’ŒæŒç»­éƒ¨ç½²æµç¨‹",
            "ç›‘æ§ç³»ç»Ÿæ€§èƒ½æŒ‡æ ‡å¹¶åŠæ—¶ä¼˜åŒ–",
            "å»ºç«‹å®Œå–„çš„æ—¥å¿—è®°å½•å’Œç›‘æ§ä½“ç³»",
            "å®šæœŸæ›´æ–°ç³»ç»Ÿä¾èµ–å’Œå®‰å…¨è¡¥ä¸",
            "å»ºç«‹å®Œå–„çš„æ•…éšœæ¢å¤å’Œç¾éš¾æ¢å¤æœºåˆ¶"
        ])
        
        # å»é‡
        recommendations = list(set(recommendations))
        
        return recommendations


async def main():
    """ä¸»å‡½æ•°"""
    print("æ¸¯è‚¡é‡åŒ–äº¤æ˜“AI Agentç³»ç»Ÿ - è¿è¡Œæ‰€æœ‰æµ‹è¯•")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæµ‹è¯•è¿è¡Œå™¨
        test_runner = TestRunner()
        
        # è¿è¡Œæ‰€æœ‰æµ‹è¯•
        comprehensive_report = await test_runner.run_all_tests()
        
        # æ˜¾ç¤ºç»¼åˆç»“æœ
        print("\nç»¼åˆæµ‹è¯•ç»“æœ:")
        print("-" * 40)
        summary = comprehensive_report["comprehensive_summary"]
        print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
        print(f"å®Œæˆæµ‹è¯•: {summary['completed_tests']}")
        print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
        print(f"æ€»ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.2%}")
        print(f"æ€»è€—æ—¶: {summary['total_duration']:.2f}ç§’")
        
        print("\nè¯¦ç»†æµ‹è¯•ç»“æœ:")
        print("-" * 40)
        for test_name, test_result in comprehensive_report["test_results"].items():
            status = "âœ“" if test_result["status"] == "COMPLETED" else "âœ—"
            duration = test_result.get("duration", 0)
            print(f"{status} {test_name}: {test_result['description']} (è€—æ—¶: {duration:.2f}ç§’)")
        
        print("\nç»¼åˆæ”¹è¿›å»ºè®®:")
        print("-" * 40)
        for i, recommendation in enumerate(comprehensive_report["recommendations"], 1):
            print(f"{i}. {recommendation}")
        
        # åˆ¤æ–­æ•´ä½“æµ‹è¯•æ˜¯å¦æˆåŠŸ
        if summary["overall_success_rate"] >= 0.8 and summary["failed_tests"] == 0:
            print(f"\nğŸ‰ æ‰€æœ‰æµ‹è¯•æˆåŠŸå®Œæˆï¼æ€»ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.2%}")
            return 0
        else:
            print(f"\nâŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼æ€»ä½“æˆåŠŸç‡: {summary['overall_success_rate']:.2%}")
            return 1
            
    except Exception as e:
        print(f"\nğŸ’¥ æµ‹è¯•è¿è¡Œè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        return 1


if __name__ == "__main__":
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
